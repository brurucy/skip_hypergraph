{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import attrgetter\n",
    "from sortedcontainers import SortedList as SortedcontainersSortedList, SortedDict as SortedcontainersSortedDict\n",
    "from pyskiplist import SkipList as PySkipList\n",
    "import bisect\n",
    "import math\n",
    "import random as random\n",
    "from pyroaring import BitMap\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# hack to add python modules from the src folder \n",
    "sys.path.append('../src')\n",
    "from bisect_killer import cy_monobound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome.\n",
    "\n",
    "In this notebook there are 4 algorithms:\n",
    "\n",
    "* Roaring Teleport Lists\n",
    "* Roaring Split List\n",
    "* Split List\n",
    "* Monobound Split List\n",
    "\n",
    "**Roaring** refers to the usage of [Roaring Bitmaps](https://arxiv.org/pdf/1603.06549.pdf) as indexes.\n",
    "\n",
    "That are compared with `sortedcontainers`'s SortedList.\n",
    "\n",
    "## Invariants\n",
    "\n",
    "All data structures have the following invariants:\n",
    "\n",
    "* Geometrically distributed random heights that hold both the data and the indexes\n",
    "* Clever use of min and max values in order to speed up lookups and inserts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Roaring MinMaxDict\n",
    "\n",
    "This is the container that will hold both the indexes and the data within each \"height\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RoaringMinMaxBitmap():\n",
    "    def __init__(self, *arg, **kwargs):\n",
    "        #super().__init__()\n",
    "        self.indexes = BitMap()\n",
    "        self.max = float('-inf')\n",
    "        self.min = float('inf')\n",
    "\n",
    "    def insert(self, key, value=None):\n",
    "        self.indexes.add(key)\n",
    "        self.max = self.indexes.max()\n",
    "        self.min = self.indexes.min()\n",
    "\n",
    "    ## Discards a VALUE/wipes it out of the index and dict\n",
    "    def discard(self, key):\n",
    "        #self.pop(key)\n",
    "        self.indexes.discard(key)\n",
    "        if not self.indexes:\n",
    "            self.max = float('-inf')\n",
    "            self.min = float('inf')\n",
    "        else:\n",
    "            self.max = self.indexes.max()\n",
    "            self.min = self.indexes.min()\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.indexes.min() < other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Teleport List\n",
    "\n",
    "This is the Teleport list, the fastest data structure we have for inserts and deletions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TeleportList(dict):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.height = -1\n",
    "        self.subindexes = []\n",
    "\n",
    "    def insert(self, key, value=None):\n",
    "\n",
    "        height = int(-(math.log2(random.random())))\n",
    "\n",
    "        if self.height < height:\n",
    "            for i in range(height - self.height):\n",
    "                self.subindexes.append(RoaringMinMaxBitmap())\n",
    "            self.height = height\n",
    "\n",
    "        highest = self.subindexes[height]\n",
    "\n",
    "        if key not in highest.indexes:\n",
    "            highest.insert(key, value)\n",
    "            dict.__setitem__(self, key, value)\n",
    "\n",
    "    def lookup(self, key):\n",
    "\n",
    "        for i in self.subindexes:\n",
    "            if i.min <= key <= i.max:\n",
    "                if key in i.indexes:\n",
    "                    return dict.__getitem__(self, key)\n",
    "        return False\n",
    "\n",
    "    def delete(self, key):\n",
    "        for i in self.subindexes:\n",
    "            if i.min <= key <= i.max:\n",
    "                if key in i.indexes:\n",
    "                    dict.__setitem__(self, key, '<deleted>')\n",
    "\n",
    "    def discard(self, key):\n",
    "        for i in self.subindexes:\n",
    "            if i.min <= key <= i.max:\n",
    "                if key in i.indexes:\n",
    "                    i.discard(key)\n",
    "                    dict.__delitem__(self, key)\n",
    "\n",
    "    def show_hedges(self):\n",
    "        for i in self.subindexes:\n",
    "            print(i.indexes)\n",
    "\n",
    "    def show_minmax(self):\n",
    "        for i in self.subindexes:\n",
    "            print(f'({i.min}, {i.max})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Roaring MaxDict\n",
    "\n",
    "This is the container that will hold both the indexes and the data within each \"height\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RoaringMaxDict:\n",
    "    def __init__(self):\n",
    "        self.indexes = BitMap()\n",
    "        self.max = float(\"-inf\")\n",
    "\n",
    "    def insert(self, key):\n",
    "        self.indexes.add(key)\n",
    "        self.max = self.indexes.max()\n",
    "\n",
    "    def discard(self, key):\n",
    "        self.indexes.discard(key)\n",
    "        if not self.indexes:\n",
    "            self.max = float('-inf')\n",
    "        else:\n",
    "            self.max = self.indexes.max()\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        if isinstance(other, int):\n",
    "            return self.max < other\n",
    "        else:\n",
    "            return self.max < other.max\n",
    "\n",
    "class SortableSubList:\n",
    "    def __init__(self):\n",
    "        self.sublists = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Split List\n",
    "\n",
    "This is the Split list, the most stable data structure we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def splitter_two(arr, load):\n",
    "    half = load // 2\n",
    "    zs = arr[0:half]\n",
    "    arr = arr.difference(zs)\n",
    "    return zs\n",
    "\n",
    "# if overload is detected, splits and adds a new split into levellist\n",
    "def Overload(blist, i, load):\n",
    "    B = RoaringMaxDict()\n",
    "    candidate_sublist = blist.sublists[i]\n",
    "    B.indexes = splitter_two(candidate_sublist.indexes, load)\n",
    "    B.max = B.indexes.max()\n",
    "    bisect.insort_left(blist.sublists, B)\n",
    "\n",
    "class RoaringSplitList(dict):\n",
    "    def __init__(self, load):\n",
    "        super().__init__()\n",
    "        self.height = -1\n",
    "        self.blists = []\n",
    "        self.load = load\n",
    "\n",
    "    def lookup(self, key):\n",
    "        for he in self.blists:\n",
    "            if key <= he.sublists[-1].max:\n",
    "                i = bisect.bisect_left(he.sublists, key)\n",
    "                if  i != len(he.sublists) and he.sublists[i].indexes[0] <= key:\n",
    "                    if key in he.sublists[i].indexes:\n",
    "                        return dict.__getitem__(self, key)\n",
    "                    \n",
    "#         print(key) #can be used to detect if the search works or not\n",
    "        return False\n",
    "\n",
    "    def insert(self, key, value=None): # Nikita added `value=None` by default\n",
    "        ## Getting the estimated geometric distribution\n",
    "        height = int(-(math.log2(random.random())))\n",
    "        ## Checking whether we need to add new edges\n",
    "        if self.height < height:\n",
    "            for i in range(height - self.height):\n",
    "                B = SortableSubList()\n",
    "                C = RoaringMaxDict()\n",
    "                B.sublists.append(C)\n",
    "                self.blists.append(B)\n",
    "            self.height = height\n",
    "\n",
    "        ## Getting the to-be-added list\n",
    "        blist = self.blists[height]\n",
    "\n",
    "        L = len(blist.sublists)\n",
    "        i = bisect.bisect_left(blist.sublists, key)\n",
    "\n",
    "        if i == 0 or L == 1:\n",
    "            updated_maxlist = blist.sublists[0].indexes\n",
    "            updated_maxlist.add(key)\n",
    "            dict.__setitem__(self, key, value)\n",
    "            blist.sublists[0].max = updated_maxlist.max()\n",
    "            if len(updated_maxlist) == self.load:\n",
    "                    Overload(blist, 0, self.load)\n",
    "        elif i == L:\n",
    "            updated_maxlist = blist.sublists[-1].indexes\n",
    "            updated_maxlist.add(key)\n",
    "            dict.__setitem__(self, key, value)\n",
    "            blist.sublists[-1].max = key\n",
    "            if len(updated_maxlist) == self.load:\n",
    "                    Overload(blist, i-1, self.load)\n",
    "\n",
    "        else:\n",
    "            updated_maxlist = blist.sublists[i].indexes\n",
    "            if updated_maxlist[0] <= key:\n",
    "                updated_maxlist.add(key)\n",
    "                dict.__setitem__(self, key, value)\n",
    "                blist.sublists[i].max = updated_maxlist.max()\n",
    "                if len(updated_maxlist) == self.load:\n",
    "                    Overload(blist, i, self.load)\n",
    "            else:\n",
    "                updated_maxlist = blist.sublists[i-1].indexes\n",
    "                updated_maxlist.add(key)\n",
    "                dict.__setitem__(self, key, value)\n",
    "                blist.sublists[i-1].max = key\n",
    "                if len(updated_maxlist) == self.load:\n",
    "                    Overload(blist, i-1, self.load)\n",
    "\n",
    "    def show_hedges(self):\n",
    "        for i in self.blists:\n",
    "            maxes = [j.max for j in i.sublists]\n",
    "            print(maxes)\n",
    "\n",
    "    def show_edges(self):\n",
    "        for i in self.blists:\n",
    "            print(\"--------\" + str(len(i.sublists)) +\"-----------\")\n",
    "            for j in i.sublists:\n",
    "                print(j.indexes)\n",
    "\n",
    "    def show_minmax(self):\n",
    "        for i in self.blists:\n",
    "            print(f'({i.min}, {i.max})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Split List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#splits the overloaded list into two consecutive parts\n",
    "def splitterSimple(arr, load):\n",
    "    half = load // 2\n",
    "    zs = [0] * half\n",
    "    for i in range(half-1, -1, -1):\n",
    "        zs[i] = arr.pop()\n",
    "    return zs\n",
    "\n",
    "def splitter2(arr, load):\n",
    "    half = load // 2 \n",
    "    res = [arr.pop() for i in range(half)]\n",
    "    res.reverse()\n",
    "    return res\n",
    "\n",
    "# if overload is detected, splits and adds a new split into levellist\n",
    "def OverloadSimple(blist, i, load):\n",
    "    B = IntervalList()\n",
    "    candidate_sublist = blist.sublists[i]\n",
    "    B.indexes = splitterSimple(candidate_sublist.indexes, load)\n",
    "    B.min = B.indexes[0]\n",
    "    #B.i = - blist.sublists[i].i - 1\n",
    "    B.max = candidate_sublist.max\n",
    "    candidate_sublist.max = candidate_sublist.indexes[-1]\n",
    "    #blist.sublists.insert(i+1, B)\n",
    "    bisect.insort_left(blist.sublists, B)\n",
    "\n",
    "class IntervalList:\n",
    "    def __init__(self):\n",
    "        self.indexes = []\n",
    "        self.max = float(\"-inf\")\n",
    "        self.min = float(\"inf\")\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        if isinstance(other, int):\n",
    "            return self.max < other\n",
    "        else:\n",
    "            return self.max < other.max\n",
    "\n",
    "class LevelList:\n",
    "    def __init__(self):\n",
    "        self.sublists = []\n",
    "        self.min = float(\"inf\")\n",
    "        self.max = float(\"-inf\")\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.max > other\n",
    "\n",
    "class SplitList:# Rucy, rename it!\n",
    "    def __init__(self, load=2000):\n",
    "        self.height = -1\n",
    "        self.blists = []\n",
    "        self.load = load\n",
    "    \n",
    "    def delete(self, nr): #not tested in this notebook\n",
    "        for he in self.blists:\n",
    "            hee = he.sublists\n",
    "            if len(hee) != 0: \n",
    "                if not( nr > hee[-1].max or nr < hee[0].min): #skipping\n",
    "                    i = bisect.bisect_left(hee, nr)\n",
    "\n",
    "                    if  i != len(hee) and not(hee[i].min > nr):\n",
    "                        j = bisect.bisect_left(hee[i].indexes, nr)\n",
    "                        if j != len(hee[i].indexes) and hee[i].indexes[j] == nr:\n",
    "                            if len(hee[i].indexes) == 1:\n",
    "                                del hee[i]\n",
    "                                \n",
    "                            else:\n",
    "                                del hee[i].indexes[j]\n",
    "                                hee[i].max = hee[i].indexes[-1]\n",
    "                                hee[i].min = hee[i].indexes[0]\n",
    "                            return True\n",
    "\n",
    "#         print(nr) #can be used to detect if the search works or not\n",
    "        return False\n",
    "    \n",
    "    def lookup(self, nr):\n",
    "\n",
    "        for he in self.blists:\n",
    "            hee = he.sublists\n",
    "            if len(hee) != 0: \n",
    "                if not( nr > hee[-1].max or nr < hee[0].min): #skipping\n",
    "                    i = bisect.bisect_left(hee, nr)\n",
    "\n",
    "                    if i != len(hee) and not(hee[i].min > nr):\n",
    "                        j = bisect.bisect_left(hee[i].indexes, nr)\n",
    "                        if j != len(hee[i].indexes) and hee[i].indexes[j] == nr:#speed up\n",
    "                            return True\n",
    "                        \n",
    "#         print(nr)\n",
    "        return False\n",
    "\n",
    "\n",
    "    def insert(self, nr):\n",
    "        ## Getting the estimated geometric distribution\n",
    "        height = int(-(math.log2(random.random())))\n",
    "        ## Checking whether we need to add new edges\n",
    "        if self.height < height:\n",
    "            for i in range(height - self.height):\n",
    "                B = LevelList()\n",
    "                C = IntervalList()\n",
    "                B.sublists.append(C)\n",
    "                self.blists.append(B)\n",
    "            self.height = height\n",
    "\n",
    "        ## Getting the to-be-added list\n",
    "        blist = self.blists[height]\n",
    "\n",
    "        ## Doing the search to see which Intervallist it should be in\n",
    "        L = len(blist.sublists)\n",
    "        i = bisect.bisect_left(blist.sublists, nr)\n",
    "\n",
    "        ## If it's smaller than all other elements then just insort it\n",
    "        if i == 0: #or L == 1:\n",
    "            candid = blist.sublists[0]\n",
    "            bisect.insort_left(candid.indexes, nr)\n",
    "            candid.max = candid.indexes[-1]\n",
    "            candid.min = candid.indexes[0]\n",
    "            \n",
    "            if len(candid.indexes) == self.load:\n",
    "                    OverloadSimple(blist, 0, self.load)\n",
    "        ## If it's bigger than all the other elements than just append it\n",
    "        elif i == L:\n",
    "            candid = blist.sublists[-1]\n",
    "            candid.indexes.append(nr)\n",
    "            candid.max = nr\n",
    "            candid.min = candid.indexes[0]\n",
    "            if len(candid.indexes) == self.load:\n",
    "                    OverloadSimple(blist, i-1, self.load)\n",
    "\n",
    "            ## Else add it\n",
    "        else:\n",
    "            candidate_sublist = blist.sublists[i]\n",
    "            # if the element is also bigger than the minimum of the current list than we insort it\n",
    "            if candidate_sublist.min <= nr:\n",
    "                bisect.insort_left(candidate_sublist.indexes, nr)\n",
    "                candidate_sublist.min = candidate_sublist.indexes[0]\n",
    "                if len(blist.sublists[i].indexes) == self.load:\n",
    "                    OverloadSimple(blist, i, self.load)\n",
    "            # then the element must be smaller then the min of the current list but therefore\n",
    "            # bigger than the max of the previous list-- so we just append it\n",
    "            else:\n",
    "                candidate_sublist = blist.sublists[i-1]\n",
    "                candidate_sublist.indexes.append(nr)\n",
    "                candidate_sublist.max = nr\n",
    "\n",
    "                if len(candidate_sublist.indexes) == self.load:\n",
    "                    OverloadSimple(blist, i-1, self.load)\n",
    "\n",
    "    def show_hedges(self):\n",
    "        for i in self.blists:\n",
    "            maxes = [j.max for j in i.sublists]\n",
    "            print(maxes)\n",
    "\n",
    "    def show_edges(self):\n",
    "        for i in self.blists:\n",
    "            print(\"--------\" + str(len(i.sublists)) +\"-----------\")\n",
    "            for j in i.sublists:\n",
    "                print(j.indexes)\n",
    "\n",
    "    def show_minmax(self):\n",
    "        for i in self.blists:\n",
    "            print(f'({i.min}, {i.max})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MonoboundSplitList\n",
    "\n",
    "SplitList with `bisect` replaced by monobound binary search implemented in Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from array import array\n",
    "\n",
    "#splits the overloaded list into two consecutive parts\n",
    "def monoboundSplitterSimple(arr, load):\n",
    "    half = load // 2\n",
    "    zs = array('l', [0]) * half\n",
    "    for i in range(half-1, -1, -1):\n",
    "        zs[i] = arr.pop()\n",
    "    return zs\n",
    "\n",
    "# if overload is detected, splits and adds a new split into levellist\n",
    "def monoboundOverloadSimple(blist, i, load):\n",
    "    B = MonoboundIntervalList()\n",
    "    candidate_sublist = blist.sublists[i]\n",
    "    B.indexes = monoboundSplitterSimple(candidate_sublist.indexes, load)\n",
    "    B.min = B.indexes[0]\n",
    "    #B.i = - blist.sublists[i].i - 1\n",
    "    B.max = candidate_sublist.max\n",
    "    candidate_sublist.max = candidate_sublist.indexes[-1]\n",
    "    #blist.sublists.insert(i+1, B)\n",
    "    bisect.insort_left(blist.sublists, B)\n",
    "\n",
    "class MonoboundIntervalList:\n",
    "    def __init__(self):\n",
    "        self.indexes = array('l')\n",
    "        self.max = float(\"-inf\")\n",
    "        self.min = float(\"inf\")\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        if isinstance(other, int):\n",
    "            return self.max < other\n",
    "        else:\n",
    "            return self.max < other.max\n",
    "\n",
    "class MonoboundSplitList:# Rucy, rename it!\n",
    "    def __init__(self, load=2000):\n",
    "        self.height = -1\n",
    "        self.blists = []\n",
    "        self.load = load\n",
    "    \n",
    "    def delete(self, nr): #not tested in this notebook\n",
    "        for he in self.blists:\n",
    "            hee = he.sublists\n",
    "            if len(hee) != 0: \n",
    "                if not( nr > hee[-1].max or nr < hee[0].min): #skipping\n",
    "                    i = bisect.bisect_left(hee, nr)\n",
    "\n",
    "                    if  i != len(hee) and not(hee[i].min > nr):\n",
    "                        j = cy_monobound.binary_search(hee[i].indexes, len(hee[i].indexes), nr)\n",
    "                        if j >= 0:\n",
    "                            if len(hee[i].indexes) == 1:\n",
    "                                del hee[i]\n",
    "                                \n",
    "                            else:\n",
    "                                del hee[i].indexes[j]\n",
    "                                hee[i].max = hee[i].indexes[-1]\n",
    "                                hee[i].min = hee[i].indexes[0]\n",
    "                            return True\n",
    "\n",
    "#         print(nr) #can be used to detect if the search works or not\n",
    "        return False\n",
    "    \n",
    "    def lookup(self, nr):\n",
    "\n",
    "        for he in self.blists:\n",
    "            hee = he.sublists\n",
    "            if len(hee) != 0: \n",
    "                if not( nr > hee[-1].max or nr < hee[0].min): #skipping\n",
    "                    i = bisect.bisect_left(hee, nr)\n",
    "\n",
    "                    if i != len(hee) and not(hee[i].min > nr):\n",
    "                        j = cy_monobound.binary_search(hee[i].indexes, len(hee[i].indexes), nr)\n",
    "                        \n",
    "                        # this binary_search implementation returns either the found index or -1, so we only need one check\n",
    "                        if j >= 0:#speed up\n",
    "                            return True\n",
    "\n",
    "        #         print(nr)\n",
    "        return False\n",
    "\n",
    "\n",
    "    def insert(self, nr):\n",
    "        ## Getting the estimated geometric distribution\n",
    "        height = int(-(math.log2(random.random())))\n",
    "        ## Checking whether we need to add new edges\n",
    "        if self.height < height:\n",
    "            for i in range(height - self.height):\n",
    "                B = LevelList()\n",
    "                C = MonoboundIntervalList()\n",
    "                B.sublists.append(C)\n",
    "                self.blists.append(B)\n",
    "            self.height = height\n",
    "\n",
    "        ## Getting the to-be-added list\n",
    "        blist = self.blists[height]\n",
    "\n",
    "        ## Doing the search to see which MonoboundIntervalList it should be in\n",
    "        L = len(blist.sublists)\n",
    "        i = bisect.bisect_left(blist.sublists, nr)\n",
    "\n",
    "        ## If it's smaller than all other elements then just insort it\n",
    "        if i == 0: #or L == 1:\n",
    "            candid = blist.sublists[0]\n",
    "            bisect.insort_left(candid.indexes, nr)\n",
    "            candid.max = candid.indexes[-1]\n",
    "            candid.min = candid.indexes[0]\n",
    "            \n",
    "            if len(candid.indexes) == self.load:\n",
    "                    monoboundOverloadSimple(blist, 0, self.load)\n",
    "        ## If it's bigger than all the other elements than just append it\n",
    "        elif i == L:\n",
    "            candid = blist.sublists[-1]\n",
    "            candid.indexes.append(nr)\n",
    "            candid.max = nr\n",
    "            candid.min = candid.indexes[0]\n",
    "            if len(candid.indexes) == self.load:\n",
    "                    monoboundOverloadSimple(blist, i-1, self.load)\n",
    "\n",
    "            ## Else add it\n",
    "        else:\n",
    "            candidate_sublist = blist.sublists[i]\n",
    "            # if the element is also bigger than the minimum of the current list than we insort it\n",
    "            if candidate_sublist.min <= nr:\n",
    "                bisect.insort_left(candidate_sublist.indexes, nr)\n",
    "                candidate_sublist.min = candidate_sublist.indexes[0]\n",
    "                if len(blist.sublists[i].indexes) == self.load:\n",
    "                    monoboundOverloadSimple(blist, i, self.load)\n",
    "            # then the element must be smaller then the min of the current list but therefore\n",
    "            # bigger than the max of the previous list-- so we just append it\n",
    "            else:\n",
    "                candidate_sublist = blist.sublists[i-1]\n",
    "                candidate_sublist.indexes.append(nr)\n",
    "                candidate_sublist.max = nr\n",
    "\n",
    "                if len(candidate_sublist.indexes) == self.load:\n",
    "                    monoboundOverloadSimple(blist, i-1, self.load)\n",
    "\n",
    "    def show_hedges(self):\n",
    "        for i in self.blists:\n",
    "            maxes = [j.max for j in i.sublists]\n",
    "            print(maxes)\n",
    "\n",
    "    def show_edges(self):\n",
    "        for i in self.blists:\n",
    "            print(\"--------\" + str(len(i.sublists)) +\"-----------\")\n",
    "            for j in i.sublists:\n",
    "                print(j.indexes)\n",
    "\n",
    "    def show_minmax(self):\n",
    "        for i in self.blists:\n",
    "            print(f'({i.min}, {i.max})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Benchmarks.\n",
    "\n",
    "Let's do some simple ones.\n",
    "\n",
    "Insert and lookup of 1 million elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.88 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "2.35 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "2.62 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "3.71 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "4.06 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "load = 10_000 #need to find optimal load\n",
    "tlist = TeleportList()\n",
    "rslist = RoaringSplitList(load)\n",
    "slist = SortedList()\n",
    "splist = SplitList()\n",
    "monobound_splist = MonoboundSplitList(load)\n",
    "\n",
    "nr = 1_000_000\n",
    "\n",
    "ten_thousand_integers = [random.randint(1, 2000000) for i in range(nr)]\n",
    "\n",
    "def insert_tlist(tl):\n",
    "    for i in range(nr):\n",
    "        tl.insert(ten_thousand_integers[i])\n",
    "\n",
    "def insert_rslist(rtl):\n",
    "    for i in range(nr):\n",
    "        rtl.insert(ten_thousand_integers[i])\n",
    "\n",
    "def insert_slist(sl):\n",
    "    for i in range(nr):\n",
    "        sl.add(ten_thousand_integers[i])\n",
    "\n",
    "def insert_nlist(novus):\n",
    "    for i in range(nr):\n",
    "        novus.insert(ten_thousand_integers[i])\n",
    "\n",
    "%timeit -r 1 -n 1 insert_tlist(tlist)\n",
    "%timeit -r 1 -n 1 insert_rslist(rslist)\n",
    "%timeit -r 1 -n 1 insert_slist(slist)\n",
    "%timeit -r 1 -n 1 insert_nlist(splist)\n",
    "%timeit -r 1 -n 1 insert_nlist(monobound_splist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "908 ms ± 9.4 ms per loop (mean ± std. dev. of 10 runs, 1 loop each)\n",
      "2 s ± 215 µs per loop (mean ± std. dev. of 2 runs, 1 loop each)\n",
      "1.96 s ± 428 µs per loop (mean ± std. dev. of 2 runs, 1 loop each)\n",
      "5.74 s ± 14.2 ms per loop (mean ± std. dev. of 2 runs, 1 loop each)\n",
      "2.21 s ± 56.8 ms per loop (mean ± std. dev. of 2 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def lookup_tlist(tl):\n",
    "    for i in range(nr):\n",
    "        tl.lookup(ten_thousand_integers[i])\n",
    "\n",
    "def lookup_rslist(rtl):\n",
    "    for i in range(nr):\n",
    "        rtl.lookup(ten_thousand_integers[i])\n",
    "\n",
    "def lookup_slist(sl):\n",
    "    for i in range(nr):\n",
    "        ten_thousand_integers[i] in sl\n",
    "\n",
    "def lookup_nlist(novus):\n",
    "    for i in range(nr):\n",
    "        novus.lookup(ten_thousand_integers[i])\n",
    "\n",
    "%timeit -r 10 -n 1 lookup_tlist(tlist)\n",
    "%timeit -r 2 -n 1 lookup_rslist(rslist)\n",
    "%timeit -r 2 -n 1 lookup_slist(slist)\n",
    "%timeit -r 2 -n 1 lookup_nlist(splist)\n",
    "%timeit -r 2 -n 1 lookup_nlist(monobound_splist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MonoboundSplitList vs SortedContainers, Different load factors:\n",
    "\n",
    "2,000 : 30% slower insert, 30% slower lookup\n",
    "\n",
    "10,000: 44% slower insert, 0 to 13 % slower lookup\n",
    "\n",
    "25,000: 84% slower insert, 0.9% slower lookup\n",
    "\n",
    "35,000: 119% slower insert, 6% **faster** lookup\n",
    "\n",
    "50,000: 260% slower insert, 10% **faster** lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing more loading factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loads = [i * 100 for i in range(1, 210, 20)] # from 100 to 20100\n",
    "\n",
    "for load in loads:\n",
    "    print(f'Testing new load: {load}...')\n",
    "    splist_monobound = MonoboundSplitList(load=load)\n",
    "#     splist_interpolated = SplitListInterpolated(load=load) # removed\n",
    "    splist = SplitList(load=load)\n",
    "    \n",
    "    insert_nlist(splist_monobound)\n",
    "#     insert_nlist(splist_interpolated) # removed\n",
    "    insert_nlist(splist)\n",
    "    \n",
    "    print('Monobound:    ', end='')\n",
    "    %timeit -r 5 -n 2 lookup_nlist(splist_monobound)   \n",
    "#     print('Interpolated: ', end='') # removed\n",
    "#     %timeit -r 5 -n 2 lookup_nlist(splist_interpolated) # removed\n",
    "    print('Bisect:       ', end='')\n",
    "    %timeit -r 5 -n 2 lookup_nlist(splist)\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loads = [2_000, 10_000, 14_000, 16_000, 18_000] # more specific loads, finding the optimal binary search + bucket\n",
    "\n",
    "for load in loads:\n",
    "    print(f'Testing new load: {load}...')\n",
    "    splist_monobound = MonoboundSplitList(load=load)\n",
    "#     splist_interpolated = SplitListInterpolated(load=load) # removed\n",
    "    \n",
    "    insert_nlist(splist_monobound)\n",
    "#     insert_nlist(splist_interpolated) # removed\n",
    "    \n",
    "    print('Monobound:    ', end='')\n",
    "    %timeit -r 10 -n 5 lookup_nlist(splist_monobound)   \n",
    "#     print('Interpolated: ', end='')\n",
    "#     %timeit -r 10 -n 5 lookup_nlist(splist_interpolated) # removed\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loads = [50_000, 100_000, 200_000, 500_000, 1_000_000] # just some random huge bucket sizes\n",
    "\n",
    "for load in loads:\n",
    "    print(f'Testing new load: {load}...')\n",
    "    splist_monobound = MonoboundSplitList(load=load)\n",
    "#     splist_interpolated = SplitListInterpolated(load=load) # removed\n",
    "    \n",
    "    insert_nlist(splist_monobound)\n",
    "#     insert_nlist(splist_interpolated) # removed\n",
    "    \n",
    "    print('Monobound:    ', end='')\n",
    "    %timeit -r 10 -n 5 lookup_nlist(splist_monobound)   \n",
    "#     print('Interpolated: ', end='')\n",
    "#     %timeit -r 10 -n 5 lookup_nlist(splist_interpolated) # removed\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maybe useful for benchmarking\n",
    "#nr = 100000\n",
    "\n",
    "def ten_k_novus_s(novus):\n",
    "    for i in range(nr):\n",
    "        novus.lookup(ten_thousand_integers[i])\n",
    "\n",
    "def ten_k_hg_s(HG):\n",
    "    for i in range(nr):\n",
    "        HG.lookup(ten_thousand_integers[i])\n",
    "\n",
    "def ten_k_sl_s(sl):\n",
    "    for i in range(nr):\n",
    "        ten_thousand_integers[i] in sl\n",
    "\n",
    "def ten_k_skeep_s(skeep):\n",
    "    for i in range(nr):\n",
    "        ten_thousand_integers[i] in skeep\n",
    "\n",
    "lon = []\n",
    "los = []\n",
    "loskip = []\n",
    "lot = []\n",
    "n = 1\n",
    "for j in range(n):\n",
    "    timesnovus = []\n",
    "    timess = []\n",
    "    timestele = []\n",
    "    timesskip = []\n",
    "    for k in range(100, 100000, 500):\n",
    "        #nlist = NovusList(i)\n",
    "        nr = k\n",
    "        hgraph = TeleportList()\n",
    "        skeepFast = SkipList()\n",
    "        slist = SortedList()\n",
    "        nlist = NovusList(2000)\n",
    "        random.seed(j)\n",
    "        ten_thousand_integers = [random.randint(1, 1000000) for i in range(nr)]\n",
    "        #ten_k_novus(nlist, ten_thousand_integers)  \n",
    "        #ten_k_hg(hgraph, ten_thousand_integers)\n",
    "        #ten_k_sl(slist, ten_thousand_integers)\n",
    "        #ten_k_skeep(skeepFast, ten_thousand_integers)\n",
    "        #ten_thousand_integers = [random.randint(1, 1000000) for i in range(nr)]\n",
    "        t1 = %timeit -q -o -r 1 -n 1 ten_k_novus(nlist,ten_thousand_integers)\n",
    "        t2 = %timeit -q -o -r 1 -n 1 ten_k_hg(hgraph,ten_thousand_integers)\n",
    "        t3 = %timeit -q -o -r 1 -n 1 ten_k_sl(slist,ten_thousand_integers)\n",
    "        t4 = %timeit -q -o -r 1 -n 1 ten_k_skeep(skeepFast,ten_thousand_integers)\n",
    "        timesnovus.append(t1.best)\n",
    "        timess.append(t2.best)\n",
    "        timestele.append(t3.best)\n",
    "        timesskip.append(t4.best)\n",
    "        \n",
    "    if j == 0:\n",
    "        lon = np.array(timesnovus)\n",
    "        los = np.array(timess)\n",
    "        lot = np.array(timestele)\n",
    "        loskip = np.array(timesskip)\n",
    "        \n",
    "    else:\n",
    "        lon += np.array(timesnovus)\n",
    "        los += np.array(timess)\n",
    "        lot += np.array(timestele)\n",
    "        loskip += np.array(timesskip)\n",
    "lon /= n\n",
    "los /= n\n",
    "lot /= n\n",
    "loskip /= n\n",
    "#print(sum(times)/n)\n",
    "plt.plot(np.arange(100,100000,500), lon, label=\"novus\")\n",
    "plt.plot(np.arange(100,100000,500), los,  label=\"tele\")\n",
    "plt.plot(np.arange(100,100000,500), lot,  label=\"sc\")\n",
    "plt.plot(np.arange(100,100000,500), loskip,  label=\"skip\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"time\")\n",
    "plt.xlabel(\"size\")\n",
    "plt.title(\"time effectivness\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing more loading factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing new load: 100...\n",
      "Monobound:    286 ms ± 5.23 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "Interpolated: 280 ms ± 1e+03 µs per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "Bisect:       562 ms ± 5.54 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "\n",
      "\n",
      "Testing new load: 2100...\n",
      "Monobound:    198 ms ± 2.67 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "Interpolated: 200 ms ± 2.36 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "Bisect:       424 ms ± 3.28 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "\n",
      "\n",
      "Testing new load: 4100...\n",
      "Monobound:    184 ms ± 977 µs per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "Interpolated: 186 ms ± 2.75 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "Bisect:       473 ms ± 70.6 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "\n",
      "\n",
      "Testing new load: 6100...\n",
      "Monobound:    181 ms ± 4.33 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "Interpolated: 185 ms ± 1.55 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "Bisect:       405 ms ± 13.7 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "\n",
      "\n",
      "Testing new load: 8100...\n",
      "Monobound:    179 ms ± 6.51 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "Interpolated: 185 ms ± 7.68 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "Bisect:       445 ms ± 43.4 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "\n",
      "\n",
      "Testing new load: 10100...\n",
      "Monobound:    180 ms ± 12 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "Interpolated: 196 ms ± 19 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "Bisect:       452 ms ± 33 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "\n",
      "\n",
      "Testing new load: 12100...\n",
      "Monobound:    189 ms ± 17.7 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "Interpolated: 183 ms ± 13.5 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "Bisect:       405 ms ± 4.62 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "\n",
      "\n",
      "Testing new load: 14100...\n",
      "Monobound:    164 ms ± 4.22 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "Interpolated: 174 ms ± 3.24 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "Bisect:       441 ms ± 46.9 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "\n",
      "\n",
      "Testing new load: 16100...\n",
      "Monobound:    198 ms ± 54.1 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "Interpolated: 170 ms ± 7.61 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "Bisect:       376 ms ± 9.71 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "\n",
      "\n",
      "Testing new load: 18100...\n",
      "Monobound:    159 ms ± 1.91 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "Interpolated: 167 ms ± 4.9 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "Bisect:       371 ms ± 5.36 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "\n",
      "\n",
      "Testing new load: 20100...\n",
      "Monobound:    159 ms ± 1.89 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "Interpolated: 176 ms ± 9.05 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "Bisect:       394 ms ± 64.4 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loads = [i * 100 for i in range(1, 210, 20)] # from 100 to 20100\n",
    "\n",
    "for load in loads:\n",
    "    print(f'Testing new load: {load}...')\n",
    "    splist_monobound = MonoboundSplitList(load=load)\n",
    "#     splist_interpolated = SplitListInterpolated(load=load) # removed\n",
    "    splist = SplitList(load=load)\n",
    "    \n",
    "    insert_nlist(splist_monobound)\n",
    "#     insert_nlist(splist_interpolated) # removed\n",
    "    insert_nlist(splist)\n",
    "    \n",
    "    print('Monobound:    ', end='')\n",
    "    %timeit -r 5 -n 2 lookup_nlist(splist_monobound)   \n",
    "#     print('Interpolated: ', end='') # removed\n",
    "#     %timeit -r 5 -n 2 lookup_nlist(splist_interpolated) # removed\n",
    "    print('Bisect:       ', end='')\n",
    "    %timeit -r 5 -n 2 lookup_nlist(splist)\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing new load: 2000...\n",
      "Monobound:    196 ms ± 1.68 ms per loop (mean ± std. dev. of 10 runs, 5 loops each)\n",
      "Interpolated: 197 ms ± 1.79 ms per loop (mean ± std. dev. of 10 runs, 5 loops each)\n",
      "\n",
      "\n",
      "Testing new load: 10000...\n",
      "Monobound:    171 ms ± 3.31 ms per loop (mean ± std. dev. of 10 runs, 5 loops each)\n",
      "Interpolated: 173 ms ± 2.12 ms per loop (mean ± std. dev. of 10 runs, 5 loops each)\n",
      "\n",
      "\n",
      "Testing new load: 14000...\n",
      "Monobound:    159 ms ± 3.72 ms per loop (mean ± std. dev. of 10 runs, 5 loops each)\n",
      "Interpolated: 163 ms ± 1.83 ms per loop (mean ± std. dev. of 10 runs, 5 loops each)\n",
      "\n",
      "\n",
      "Testing new load: 16000...\n",
      "Monobound:    154 ms ± 1.42 ms per loop (mean ± std. dev. of 10 runs, 5 loops each)\n",
      "Interpolated: 162 ms ± 2.24 ms per loop (mean ± std. dev. of 10 runs, 5 loops each)\n",
      "\n",
      "\n",
      "Testing new load: 18000...\n",
      "Monobound:    156 ms ± 2.5 ms per loop (mean ± std. dev. of 10 runs, 5 loops each)\n",
      "Interpolated: 161 ms ± 2.32 ms per loop (mean ± std. dev. of 10 runs, 5 loops each)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loads = [2_000, 10_000, 14_000, 16_000, 18_000] # more specific loads, finding the optimal binary search + bucket\n",
    "\n",
    "for load in loads:\n",
    "    print(f'Testing new load: {load}...')\n",
    "    splist_monobound = MonoboundSplitList(load=load)\n",
    "#     splist_interpolated = SplitListInterpolated(load=load) # removed\n",
    "    \n",
    "    insert_nlist(splist_monobound)\n",
    "#     insert_nlist(splist_interpolated) # removed\n",
    "    \n",
    "    print('Monobound:    ', end='')\n",
    "    %timeit -r 10 -n 5 lookup_nlist(splist_monobound)   \n",
    "#     print('Interpolated: ', end='')\n",
    "#     %timeit -r 10 -n 5 lookup_nlist(splist_interpolated) # removed\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing new load: 50000...\n",
      "Monobound:    131 ms ± 2.41 ms per loop (mean ± std. dev. of 10 runs, 5 loops each)\n",
      "Interpolated: 142 ms ± 4.99 ms per loop (mean ± std. dev. of 10 runs, 5 loops each)\n",
      "\n",
      "\n",
      "Testing new load: 100000...\n",
      "Monobound:    135 ms ± 2.94 ms per loop (mean ± std. dev. of 10 runs, 5 loops each)\n",
      "Interpolated: 141 ms ± 20.5 ms per loop (mean ± std. dev. of 10 runs, 5 loops each)\n",
      "\n",
      "\n",
      "Testing new load: 200000...\n",
      "Monobound:    160 ms ± 23.4 ms per loop (mean ± std. dev. of 10 runs, 5 loops each)\n",
      "Interpolated: 138 ms ± 2.31 ms per loop (mean ± std. dev. of 10 runs, 5 loops each)\n",
      "\n",
      "\n",
      "Testing new load: 500000...\n",
      "Monobound:    134 ms ± 1.69 ms per loop (mean ± std. dev. of 10 runs, 5 loops each)\n",
      "Interpolated: 136 ms ± 1.64 ms per loop (mean ± std. dev. of 10 runs, 5 loops each)\n",
      "\n",
      "\n",
      "Testing new load: 1000000...\n",
      "Monobound:    133 ms ± 3.75 ms per loop (mean ± std. dev. of 10 runs, 5 loops each)\n",
      "Interpolated: 136 ms ± 2.25 ms per loop (mean ± std. dev. of 10 runs, 5 loops each)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loads = [50_000, 100_000, 200_000, 500_000, 1_000_000] # just some random huge bucket sizes\n",
    "\n",
    "for load in loads:\n",
    "    print(f'Testing new load: {load}...')\n",
    "    splist_monobound = MonoboundSplitList(load=load)\n",
    "#     splist_interpolated = SplitListInterpolated(load=load) # removed\n",
    "    \n",
    "    insert_nlist(splist_monobound)\n",
    "#     insert_nlist(splist_interpolated) # removed\n",
    "    \n",
    "    print('Monobound:    ', end='')\n",
    "    %timeit -r 10 -n 5 lookup_nlist(splist_monobound)   \n",
    "#     print('Interpolated: ', end='')\n",
    "#     %timeit -r 10 -n 5 lookup_nlist(splist_interpolated) # removed\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Official Benchmarks\n",
    "\n",
    "We run comparisons for two abstract data structures:\n",
    "\n",
    "`SortedList` - SplitList, MonoboundSplitList, SkipList from `pyskiplist`, SortedList from `sortedcontainers`\n",
    "\n",
    "`SortedDict` - RoaringSplitList, RoaringTeleportList, SortedDict from `sortedcontainers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SortedList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT\n",
    "\n",
    "def insert_sortedcontainers_list(lst, data):\n",
    "    for el in data:\n",
    "        _ = lst.add(el)\n",
    "        \n",
    "def insert_sortedcontainers_dict(dct, data):\n",
    "    for el in data:\n",
    "        _ = dct.setdefault(el)\n",
    "\n",
    "def insert_our(lst, data):\n",
    "    for el in data:\n",
    "        _ = lst.insert(el)\n",
    "        \n",
    "def insert_pyskiplist(lst, data):\n",
    "    for el in data:\n",
    "        _ = lst.insert(el, el)\n",
    "        \n",
    "# LOOKUP\n",
    "\n",
    "def lookup_sortedcontainers_list(lst, data):\n",
    "    for el in data:\n",
    "        _ = el in lst\n",
    "        \n",
    "def lookup_sortedcontainers_dict(dct, data):\n",
    "    for el in data:\n",
    "        _ = dct.get(el)\n",
    "\n",
    "def lookup_our(lst, data):\n",
    "    for el in data:\n",
    "        _ = lst.lookup(el)\n",
    "        \n",
    "def lookup_pyskiplist(lst, data):\n",
    "    for el in data:\n",
    "        _ = lst.search(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Trial 1/600. Seed: 0. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 2/600. Seed: 1. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 3/600. Seed: 2. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 4/600. Seed: 3. Data size: 10. Running... Done: 0.13s --\n",
      "-- Trial 5/600. Seed: 4. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 6/600. Seed: 5. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 7/600. Seed: 6. Data size: 10. Running... Done: 0.13s --\n",
      "-- Trial 8/600. Seed: 7. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 9/600. Seed: 8. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 10/600. Seed: 9. Data size: 10. Running... Done: 0.13s --\n",
      "-- Trial 11/600. Seed: 10. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 12/600. Seed: 11. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 13/600. Seed: 12. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 14/600. Seed: 13. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 15/600. Seed: 14. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 16/600. Seed: 15. Data size: 10. Running... Done: 0.13s --\n",
      "-- Trial 17/600. Seed: 16. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 18/600. Seed: 17. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 19/600. Seed: 18. Data size: 10. Running... Done: 0.13s --\n",
      "-- Trial 20/600. Seed: 19. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 21/600. Seed: 20. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 22/600. Seed: 21. Data size: 10. Running... Done: 0.13s --\n",
      "-- Trial 23/600. Seed: 22. Data size: 10. Running... Done: 0.14s --\n",
      "-- Trial 24/600. Seed: 23. Data size: 10. Running... Done: 0.14s --\n",
      "-- Trial 25/600. Seed: 24. Data size: 10. Running... Done: 0.13s --\n",
      "-- Trial 26/600. Seed: 25. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 27/600. Seed: 26. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 28/600. Seed: 27. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 29/600. Seed: 28. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 30/600. Seed: 29. Data size: 10. Running... Done: 0.13s --\n",
      "-- Trial 31/600. Seed: 30. Data size: 10. Running... Done: 0.13s --\n",
      "-- Trial 32/600. Seed: 31. Data size: 10. Running... Done: 0.13s --\n",
      "-- Trial 33/600. Seed: 32. Data size: 10. Running... Done: 0.13s --\n",
      "-- Trial 34/600. Seed: 33. Data size: 10. Running... Done: 0.13s --\n",
      "-- Trial 35/600. Seed: 34. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 36/600. Seed: 35. Data size: 10. Running... Done: 0.14s --\n",
      "-- Trial 37/600. Seed: 36. Data size: 10. Running... Done: 0.13s --\n",
      "-- Trial 38/600. Seed: 37. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 39/600. Seed: 38. Data size: 10. Running... Done: 0.14s --\n",
      "-- Trial 40/600. Seed: 39. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 41/600. Seed: 40. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 42/600. Seed: 41. Data size: 10. Running... Done: 0.13s --\n",
      "-- Trial 43/600. Seed: 42. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 44/600. Seed: 43. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 45/600. Seed: 44. Data size: 10. Running... Done: 0.13s --\n",
      "-- Trial 46/600. Seed: 45. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 47/600. Seed: 46. Data size: 10. Running... Done: 0.13s --\n",
      "-- Trial 48/600. Seed: 47. Data size: 10. Running... Done: 0.13s --\n",
      "-- Trial 49/600. Seed: 48. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 50/600. Seed: 49. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 51/600. Seed: 50. Data size: 10. Running... Done: 0.13s --\n",
      "-- Trial 52/600. Seed: 51. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 53/600. Seed: 52. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 54/600. Seed: 53. Data size: 10. Running... Done: 0.13s --\n",
      "-- Trial 55/600. Seed: 54. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 56/600. Seed: 55. Data size: 10. Running... Done: 0.13s --\n",
      "-- Trial 57/600. Seed: 56. Data size: 10. Running... Done: 0.14s --\n",
      "-- Trial 58/600. Seed: 57. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 59/600. Seed: 58. Data size: 10. Running... Done: 0.13s --\n",
      "-- Trial 60/600. Seed: 59. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 61/600. Seed: 60. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 62/600. Seed: 61. Data size: 10. Running... Done: 0.13s --\n",
      "-- Trial 63/600. Seed: 62. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 64/600. Seed: 63. Data size: 10. Running... Done: 0.13s --\n",
      "-- Trial 65/600. Seed: 64. Data size: 10. Running... Done: 0.13s --\n",
      "-- Trial 66/600. Seed: 65. Data size: 10. Running... Done: 0.14s --\n",
      "-- Trial 67/600. Seed: 66. Data size: 10. Running... Done: 0.14s --\n",
      "-- Trial 68/600. Seed: 67. Data size: 10. Running... Done: 0.15s --\n",
      "-- Trial 69/600. Seed: 68. Data size: 10. Running... Done: 0.14s --\n",
      "-- Trial 70/600. Seed: 69. Data size: 10. Running... Done: 0.13s --\n",
      "-- Trial 71/600. Seed: 70. Data size: 10. Running... Done: 0.15s --\n",
      "-- Trial 72/600. Seed: 71. Data size: 10. Running... Done: 0.14s --\n",
      "-- Trial 73/600. Seed: 72. Data size: 10. Running... Done: 0.14s --\n",
      "-- Trial 74/600. Seed: 73. Data size: 10. Running... Done: 0.14s --\n",
      "-- Trial 75/600. Seed: 74. Data size: 10. Running... Done: 0.13s --\n",
      "-- Trial 76/600. Seed: 75. Data size: 10. Running... Done: 0.14s --\n",
      "-- Trial 77/600. Seed: 76. Data size: 10. Running... Done: 0.13s --\n",
      "-- Trial 78/600. Seed: 77. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 79/600. Seed: 78. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 80/600. Seed: 79. Data size: 10. Running... Done: 0.13s --\n",
      "-- Trial 81/600. Seed: 80. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 82/600. Seed: 81. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 83/600. Seed: 82. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 84/600. Seed: 83. Data size: 10. Running... Done: 0.13s --\n",
      "-- Trial 85/600. Seed: 84. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 86/600. Seed: 85. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 87/600. Seed: 86. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 88/600. Seed: 87. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 89/600. Seed: 88. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 90/600. Seed: 89. Data size: 10. Running... Done: 0.13s --\n",
      "-- Trial 91/600. Seed: 90. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 92/600. Seed: 91. Data size: 10. Running... Done: 0.12s --\n",
      "-- Trial 93/600. Seed: 92. Data size: 10. Running... Done: 0.13s --\n",
      "-- Trial 94/600. Seed: 93. Data size: 10. Running... Done: 0.18s --\n",
      "-- Trial 95/600. Seed: 94. Data size: 10. Running... Done: 0.16s --\n",
      "-- Trial 96/600. Seed: 95. Data size: 10. Running... Done: 0.16s --\n",
      "-- Trial 97/600. Seed: 96. Data size: 10. Running... Done: 0.16s --\n",
      "-- Trial 98/600. Seed: 97. Data size: 10. Running... Done: 0.16s --\n",
      "-- Trial 99/600. Seed: 98. Data size: 10. Running... Done: 0.16s --\n",
      "-- Trial 100/600. Seed: 99. Data size: 10. Running... Done: 0.16s --\n",
      "-- Trial 101/600. Seed: 0. Data size: 100. Running... Done: 0.16s --\n",
      "-- Trial 102/600. Seed: 1. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 103/600. Seed: 2. Data size: 100. Running... Done: 0.16s --\n",
      "-- Trial 104/600. Seed: 3. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 105/600. Seed: 4. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 106/600. Seed: 5. Data size: 100. Running... Done: 0.16s --\n",
      "-- Trial 107/600. Seed: 6. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 108/600. Seed: 7. Data size: 100. Running... Done: 0.16s --\n",
      "-- Trial 109/600. Seed: 8. Data size: 100. Running... Done: 0.16s --\n",
      "-- Trial 110/600. Seed: 9. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 111/600. Seed: 10. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 112/600. Seed: 11. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 113/600. Seed: 12. Data size: 100. Running... Done: 0.16s --\n",
      "-- Trial 114/600. Seed: 13. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 115/600. Seed: 14. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 116/600. Seed: 15. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 117/600. Seed: 16. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 118/600. Seed: 17. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 119/600. Seed: 18. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 120/600. Seed: 19. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 121/600. Seed: 20. Data size: 100. Running... Done: 0.16s --\n",
      "-- Trial 122/600. Seed: 21. Data size: 100. Running... Done: 0.16s --\n",
      "-- Trial 123/600. Seed: 22. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 124/600. Seed: 23. Data size: 100. Running... Done: 0.16s --\n",
      "-- Trial 125/600. Seed: 24. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 126/600. Seed: 25. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 127/600. Seed: 26. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 128/600. Seed: 27. Data size: 100. Running... Done: 0.16s --\n",
      "-- Trial 129/600. Seed: 28. Data size: 100. Running... Done: 0.16s --\n",
      "-- Trial 130/600. Seed: 29. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 131/600. Seed: 30. Data size: 100. Running... Done: 0.16s --\n",
      "-- Trial 132/600. Seed: 31. Data size: 100. Running... Done: 0.18s --\n",
      "-- Trial 133/600. Seed: 32. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 134/600. Seed: 33. Data size: 100. Running... Done: 0.16s --\n",
      "-- Trial 135/600. Seed: 34. Data size: 100. Running... Done: 0.16s --\n",
      "-- Trial 136/600. Seed: 35. Data size: 100. Running... Done: 0.16s --\n",
      "-- Trial 137/600. Seed: 36. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 138/600. Seed: 37. Data size: 100. Running... Done: 0.16s --\n",
      "-- Trial 139/600. Seed: 38. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 140/600. Seed: 39. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 141/600. Seed: 40. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 142/600. Seed: 41. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 143/600. Seed: 42. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 144/600. Seed: 43. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 145/600. Seed: 44. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 146/600. Seed: 45. Data size: 100. Running... Done: 0.16s --\n",
      "-- Trial 147/600. Seed: 46. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 148/600. Seed: 47. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 149/600. Seed: 48. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 150/600. Seed: 49. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 151/600. Seed: 50. Data size: 100. Running... Done: 0.16s --\n",
      "-- Trial 152/600. Seed: 51. Data size: 100. Running... Done: 0.16s --\n",
      "-- Trial 153/600. Seed: 52. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 154/600. Seed: 53. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 155/600. Seed: 54. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 156/600. Seed: 55. Data size: 100. Running... Done: 0.16s --\n",
      "-- Trial 157/600. Seed: 56. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 158/600. Seed: 57. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 159/600. Seed: 58. Data size: 100. Running... Done: 0.12s --\n",
      "-- Trial 160/600. Seed: 59. Data size: 100. Running... Done: 0.13s --\n",
      "-- Trial 161/600. Seed: 60. Data size: 100. Running... Done: 0.13s --\n",
      "-- Trial 162/600. Seed: 61. Data size: 100. Running... Done: 0.12s --\n",
      "-- Trial 163/600. Seed: 62. Data size: 100. Running... Done: 0.13s --\n",
      "-- Trial 164/600. Seed: 63. Data size: 100. Running... Done: 0.13s --\n",
      "-- Trial 165/600. Seed: 64. Data size: 100. Running... Done: 0.13s --\n",
      "-- Trial 166/600. Seed: 65. Data size: 100. Running... Done: 0.13s --\n",
      "-- Trial 167/600. Seed: 66. Data size: 100. Running... Done: 0.13s --\n",
      "-- Trial 168/600. Seed: 67. Data size: 100. Running... Done: 0.13s --\n",
      "-- Trial 169/600. Seed: 68. Data size: 100. Running... Done: 0.13s --\n",
      "-- Trial 170/600. Seed: 69. Data size: 100. Running... Done: 0.14s --\n",
      "-- Trial 171/600. Seed: 70. Data size: 100. Running... Done: 0.13s --\n",
      "-- Trial 172/600. Seed: 71. Data size: 100. Running... Done: 0.13s --\n",
      "-- Trial 173/600. Seed: 72. Data size: 100. Running... Done: 0.14s --\n",
      "-- Trial 174/600. Seed: 73. Data size: 100. Running... Done: 0.13s --\n",
      "-- Trial 175/600. Seed: 74. Data size: 100. Running... Done: 0.13s --\n",
      "-- Trial 176/600. Seed: 75. Data size: 100. Running... Done: 0.13s --\n",
      "-- Trial 177/600. Seed: 76. Data size: 100. Running... Done: 0.13s --\n",
      "-- Trial 178/600. Seed: 77. Data size: 100. Running... Done: 0.13s --\n",
      "-- Trial 179/600. Seed: 78. Data size: 100. Running... Done: 0.13s --\n",
      "-- Trial 180/600. Seed: 79. Data size: 100. Running... Done: 0.13s --\n",
      "-- Trial 181/600. Seed: 80. Data size: 100. Running... Done: 0.13s --\n",
      "-- Trial 182/600. Seed: 81. Data size: 100. Running... Done: 0.13s --\n",
      "-- Trial 183/600. Seed: 82. Data size: 100. Running... Done: 0.13s --\n",
      "-- Trial 184/600. Seed: 83. Data size: 100. Running... Done: 0.14s --\n",
      "-- Trial 185/600. Seed: 84. Data size: 100. Running... Done: 0.13s --\n",
      "-- Trial 186/600. Seed: 85. Data size: 100. Running... Done: 0.14s --\n",
      "-- Trial 187/600. Seed: 86. Data size: 100. Running... Done: 0.13s --\n",
      "-- Trial 188/600. Seed: 87. Data size: 100. Running... Done: 0.13s --\n",
      "-- Trial 189/600. Seed: 88. Data size: 100. Running... Done: 0.17s --\n",
      "-- Trial 190/600. Seed: 89. Data size: 100. Running... Done: 0.12s --\n",
      "-- Trial 191/600. Seed: 90. Data size: 100. Running... Done: 0.13s --\n",
      "-- Trial 192/600. Seed: 91. Data size: 100. Running... Done: 0.13s --\n",
      "-- Trial 193/600. Seed: 92. Data size: 100. Running... Done: 0.14s --\n",
      "-- Trial 194/600. Seed: 93. Data size: 100. Running... Done: 0.14s --\n",
      "-- Trial 195/600. Seed: 94. Data size: 100. Running... Done: 0.14s --\n",
      "-- Trial 196/600. Seed: 95. Data size: 100. Running... Done: 0.13s --\n",
      "-- Trial 197/600. Seed: 96. Data size: 100. Running... Done: 0.14s --\n",
      "-- Trial 198/600. Seed: 97. Data size: 100. Running... Done: 0.13s --\n",
      "-- Trial 199/600. Seed: 98. Data size: 100. Running... Done: 0.13s --\n",
      "-- Trial 200/600. Seed: 99. Data size: 100. Running... Done: 0.13s --\n",
      "-- Trial 201/600. Seed: 0. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 202/600. Seed: 1. Data size: 1000. Running... Done: 0.22s --\n",
      "-- Trial 203/600. Seed: 2. Data size: 1000. Running... Done: 0.21s --\n",
      "-- Trial 204/600. Seed: 3. Data size: 1000. Running... Done: 0.21s --\n",
      "-- Trial 205/600. Seed: 4. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 206/600. Seed: 5. Data size: 1000. Running... Done: 0.22s --\n",
      "-- Trial 207/600. Seed: 6. Data size: 1000. Running... Done: 0.21s --\n",
      "-- Trial 208/600. Seed: 7. Data size: 1000. Running... Done: 0.24s --\n",
      "-- Trial 209/600. Seed: 8. Data size: 1000. Running... Done: 0.20s --\n",
      "-- Trial 210/600. Seed: 9. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 211/600. Seed: 10. Data size: 1000. Running... Done: 0.20s --\n",
      "-- Trial 212/600. Seed: 11. Data size: 1000. Running... Done: 0.21s --\n",
      "-- Trial 213/600. Seed: 12. Data size: 1000. Running... Done: 0.20s --\n",
      "-- Trial 214/600. Seed: 13. Data size: 1000. Running... Done: 0.21s --\n",
      "-- Trial 215/600. Seed: 14. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 216/600. Seed: 15. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 217/600. Seed: 16. Data size: 1000. Running... Done: 0.24s --\n",
      "-- Trial 218/600. Seed: 17. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 219/600. Seed: 18. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 220/600. Seed: 19. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 221/600. Seed: 20. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 222/600. Seed: 21. Data size: 1000. Running... Done: 0.24s --\n",
      "-- Trial 223/600. Seed: 22. Data size: 1000. Running... Done: 0.20s --\n",
      "-- Trial 224/600. Seed: 23. Data size: 1000. Running... Done: 0.20s --\n",
      "-- Trial 225/600. Seed: 24. Data size: 1000. Running... Done: 0.21s --\n",
      "-- Trial 226/600. Seed: 25. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 227/600. Seed: 26. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 228/600. Seed: 27. Data size: 1000. Running... Done: 0.24s --\n",
      "-- Trial 229/600. Seed: 28. Data size: 1000. Running... Done: 0.22s --\n",
      "-- Trial 230/600. Seed: 29. Data size: 1000. Running... Done: 0.24s --\n",
      "-- Trial 231/600. Seed: 30. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 232/600. Seed: 31. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 233/600. Seed: 32. Data size: 1000. Running... Done: 0.27s --\n",
      "-- Trial 234/600. Seed: 33. Data size: 1000. Running... Done: 0.21s --\n",
      "-- Trial 235/600. Seed: 34. Data size: 1000. Running... Done: 0.21s --\n",
      "-- Trial 236/600. Seed: 35. Data size: 1000. Running... Done: 0.20s --\n",
      "-- Trial 237/600. Seed: 36. Data size: 1000. Running... Done: 0.20s --\n",
      "-- Trial 238/600. Seed: 37. Data size: 1000. Running... Done: 0.22s --\n",
      "-- Trial 239/600. Seed: 38. Data size: 1000. Running... Done: 0.24s --\n",
      "-- Trial 240/600. Seed: 39. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 241/600. Seed: 40. Data size: 1000. Running... Done: 0.22s --\n",
      "-- Trial 242/600. Seed: 41. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 243/600. Seed: 42. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 244/600. Seed: 43. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 245/600. Seed: 44. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 246/600. Seed: 45. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 247/600. Seed: 46. Data size: 1000. Running... Done: 0.22s --\n",
      "-- Trial 248/600. Seed: 47. Data size: 1000. Running... Done: 0.22s --\n",
      "-- Trial 249/600. Seed: 48. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 250/600. Seed: 49. Data size: 1000. Running... Done: 0.22s --\n",
      "-- Trial 251/600. Seed: 50. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 252/600. Seed: 51. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 253/600. Seed: 52. Data size: 1000. Running... Done: 0.22s --\n",
      "-- Trial 254/600. Seed: 53. Data size: 1000. Running... Done: 0.26s --\n",
      "-- Trial 255/600. Seed: 54. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 256/600. Seed: 55. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 257/600. Seed: 56. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 258/600. Seed: 57. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 259/600. Seed: 58. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 260/600. Seed: 59. Data size: 1000. Running... Done: 0.22s --\n",
      "-- Trial 261/600. Seed: 60. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 262/600. Seed: 61. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 263/600. Seed: 62. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 264/600. Seed: 63. Data size: 1000. Running... Done: 0.24s --\n",
      "-- Trial 265/600. Seed: 64. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 266/600. Seed: 65. Data size: 1000. Running... Done: 0.22s --\n",
      "-- Trial 267/600. Seed: 66. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 268/600. Seed: 67. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 269/600. Seed: 68. Data size: 1000. Running... Done: 0.22s --\n",
      "-- Trial 270/600. Seed: 69. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 271/600. Seed: 70. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 272/600. Seed: 71. Data size: 1000. Running... Done: 0.22s --\n",
      "-- Trial 273/600. Seed: 72. Data size: 1000. Running... Done: 0.22s --\n",
      "-- Trial 274/600. Seed: 73. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 275/600. Seed: 74. Data size: 1000. Running... Done: 0.22s --\n",
      "-- Trial 276/600. Seed: 75. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 277/600. Seed: 76. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 278/600. Seed: 77. Data size: 1000. Running... Done: 0.22s --\n",
      "-- Trial 279/600. Seed: 78. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 280/600. Seed: 79. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 281/600. Seed: 80. Data size: 1000. Running... Done: 0.22s --\n",
      "-- Trial 282/600. Seed: 81. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 283/600. Seed: 82. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 284/600. Seed: 83. Data size: 1000. Running... Done: 0.22s --\n",
      "-- Trial 285/600. Seed: 84. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 286/600. Seed: 85. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 287/600. Seed: 86. Data size: 1000. Running... Done: 0.22s --\n",
      "-- Trial 288/600. Seed: 87. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 289/600. Seed: 88. Data size: 1000. Running... Done: 0.22s --\n",
      "-- Trial 290/600. Seed: 89. Data size: 1000. Running... Done: 0.22s --\n",
      "-- Trial 291/600. Seed: 90. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 292/600. Seed: 91. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 293/600. Seed: 92. Data size: 1000. Running... Done: 0.22s --\n",
      "-- Trial 294/600. Seed: 93. Data size: 1000. Running... Done: 0.22s --\n",
      "-- Trial 295/600. Seed: 94. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 296/600. Seed: 95. Data size: 1000. Running... Done: 0.22s --\n",
      "-- Trial 297/600. Seed: 96. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 298/600. Seed: 97. Data size: 1000. Running... Done: 0.23s --\n",
      "-- Trial 299/600. Seed: 98. Data size: 1000. Running... Done: 0.22s --\n",
      "-- Trial 300/600. Seed: 99. Data size: 1000. Running... Done: 0.22s --\n",
      "-- Trial 301/600. Seed: 0. Data size: 10000. Running... Done: 1.14s --\n",
      "-- Trial 302/600. Seed: 1. Data size: 10000. Running... Done: 1.17s --\n",
      "-- Trial 303/600. Seed: 2. Data size: 10000. Running... Done: 1.12s --\n",
      "-- Trial 304/600. Seed: 3. Data size: 10000. Running... Done: 1.16s --\n",
      "-- Trial 305/600. Seed: 4. Data size: 10000. Running... Done: 1.16s --\n",
      "-- Trial 306/600. Seed: 5. Data size: 10000. Running... Done: 1.13s --\n",
      "-- Trial 307/600. Seed: 6. Data size: 10000. Running... Done: 1.14s --\n",
      "-- Trial 308/600. Seed: 7. Data size: 10000. Running... Done: 1.14s --\n",
      "-- Trial 309/600. Seed: 8. Data size: 10000. Running... Done: 1.14s --\n",
      "-- Trial 310/600. Seed: 9. Data size: 10000. Running... Done: 1.15s --\n",
      "-- Trial 311/600. Seed: 10. Data size: 10000. Running... Done: 1.17s --\n",
      "-- Trial 312/600. Seed: 11. Data size: 10000. Running... Done: 1.21s --\n",
      "-- Trial 313/600. Seed: 12. Data size: 10000. Running... Done: 1.16s --\n",
      "-- Trial 314/600. Seed: 13. Data size: 10000. Running... Done: 1.16s --\n",
      "-- Trial 315/600. Seed: 14. Data size: 10000. Running... Done: 1.14s --\n",
      "-- Trial 316/600. Seed: 15. Data size: 10000. Running... Done: 1.12s --\n",
      "-- Trial 317/600. Seed: 16. Data size: 10000. Running... Done: 1.16s --\n",
      "-- Trial 318/600. Seed: 17. Data size: 10000. Running... Done: 1.15s --\n",
      "-- Trial 319/600. Seed: 18. Data size: 10000. Running... Done: 1.12s --\n",
      "-- Trial 320/600. Seed: 19. Data size: 10000. Running... Done: 1.14s --\n",
      "-- Trial 321/600. Seed: 20. Data size: 10000. Running... Done: 1.15s --\n",
      "-- Trial 322/600. Seed: 21. Data size: 10000. Running... Done: 1.15s --\n",
      "-- Trial 323/600. Seed: 22. Data size: 10000. Running... Done: 1.14s --\n",
      "-- Trial 324/600. Seed: 23. Data size: 10000. Running... Done: 1.13s --\n",
      "-- Trial 325/600. Seed: 24. Data size: 10000. Running... Done: 1.16s --\n",
      "-- Trial 326/600. Seed: 25. Data size: 10000. Running... Done: 1.14s --\n",
      "-- Trial 327/600. Seed: 26. Data size: 10000. Running... Done: 1.16s --\n",
      "-- Trial 328/600. Seed: 27. Data size: 10000. Running... Done: 1.15s --\n",
      "-- Trial 329/600. Seed: 28. Data size: 10000. Running... Done: 1.20s --\n",
      "-- Trial 330/600. Seed: 29. Data size: 10000. Running... Done: 1.13s --\n",
      "-- Trial 331/600. Seed: 30. Data size: 10000. Running... Done: 1.12s --\n",
      "-- Trial 332/600. Seed: 31. Data size: 10000. Running... Done: 1.14s --\n",
      "-- Trial 333/600. Seed: 32. Data size: 10000. Running... Done: 1.15s --\n",
      "-- Trial 334/600. Seed: 33. Data size: 10000. Running... Done: 1.14s --\n",
      "-- Trial 335/600. Seed: 34. Data size: 10000. Running... Done: 1.14s --\n",
      "-- Trial 336/600. Seed: 35. Data size: 10000. Running... Done: 1.14s --\n",
      "-- Trial 337/600. Seed: 36. Data size: 10000. Running... Done: 1.15s --\n",
      "-- Trial 338/600. Seed: 37. Data size: 10000. Running... Done: 1.15s --\n",
      "-- Trial 339/600. Seed: 38. Data size: 10000. Running... Done: 1.13s --\n",
      "-- Trial 340/600. Seed: 39. Data size: 10000. Running... Done: 1.15s --\n",
      "-- Trial 341/600. Seed: 40. Data size: 10000. Running... Done: 1.15s --\n",
      "-- Trial 342/600. Seed: 41. Data size: 10000. Running... Done: 1.17s --\n",
      "-- Trial 343/600. Seed: 42. Data size: 10000. Running... Done: 1.15s --\n",
      "-- Trial 344/600. Seed: 43. Data size: 10000. Running... Done: 1.15s --\n",
      "-- Trial 345/600. Seed: 44. Data size: 10000. Running... Done: 1.15s --\n",
      "-- Trial 346/600. Seed: 45. Data size: 10000. Running... Done: 1.14s --\n",
      "-- Trial 347/600. Seed: 46. Data size: 10000. Running... Done: 1.15s --\n",
      "-- Trial 348/600. Seed: 47. Data size: 10000. Running... Done: 1.16s --\n",
      "-- Trial 349/600. Seed: 48. Data size: 10000. Running... Done: 1.13s --\n",
      "-- Trial 350/600. Seed: 49. Data size: 10000. Running... Done: 1.13s --\n",
      "-- Trial 351/600. Seed: 50. Data size: 10000. Running... Done: 1.13s --\n",
      "-- Trial 352/600. Seed: 51. Data size: 10000. Running... Done: 1.14s --\n",
      "-- Trial 353/600. Seed: 52. Data size: 10000. Running... Done: 1.13s --\n",
      "-- Trial 354/600. Seed: 53. Data size: 10000. Running... Done: 1.18s --\n",
      "-- Trial 355/600. Seed: 54. Data size: 10000. Running... Done: 1.13s --\n",
      "-- Trial 356/600. Seed: 55. Data size: 10000. Running... Done: 1.14s --\n",
      "-- Trial 357/600. Seed: 56. Data size: 10000. Running... Done: 1.18s --\n",
      "-- Trial 358/600. Seed: 57. Data size: 10000. Running... Done: 1.14s --\n",
      "-- Trial 359/600. Seed: 58. Data size: 10000. Running... Done: 1.14s --\n",
      "-- Trial 360/600. Seed: 59. Data size: 10000. Running... Done: 1.15s --\n",
      "-- Trial 361/600. Seed: 60. Data size: 10000. Running... Done: 1.16s --\n",
      "-- Trial 362/600. Seed: 61. Data size: 10000. Running... Done: 1.12s --\n",
      "-- Trial 363/600. Seed: 62. Data size: 10000. Running... Done: 1.14s --\n",
      "-- Trial 364/600. Seed: 63. Data size: 10000. Running... Done: 1.15s --\n",
      "-- Trial 365/600. Seed: 64. Data size: 10000. Running... Done: 1.14s --\n",
      "-- Trial 366/600. Seed: 65. Data size: 10000. Running... Done: 1.14s --\n",
      "-- Trial 367/600. Seed: 66. Data size: 10000. Running... Done: 1.13s --\n",
      "-- Trial 368/600. Seed: 67. Data size: 10000. Running... Done: 1.14s --\n",
      "-- Trial 369/600. Seed: 68. Data size: 10000. Running... Done: 1.15s --\n",
      "-- Trial 370/600. Seed: 69. Data size: 10000. Running... Done: 1.11s --\n",
      "-- Trial 371/600. Seed: 70. Data size: 10000. Running... Done: 1.13s --\n",
      "-- Trial 372/600. Seed: 71. Data size: 10000. Running... Done: 1.16s --\n",
      "-- Trial 373/600. Seed: 72. Data size: 10000. Running... Done: 1.13s --\n",
      "-- Trial 374/600. Seed: 73. Data size: 10000. Running... Done: 1.13s --\n",
      "-- Trial 375/600. Seed: 74. Data size: 10000. Running... Done: 1.14s --\n",
      "-- Trial 376/600. Seed: 75. Data size: 10000. Running... Done: 1.17s --\n",
      "-- Trial 377/600. Seed: 76. Data size: 10000. Running... Done: 1.17s --\n",
      "-- Trial 378/600. Seed: 77. Data size: 10000. Running... Done: 1.13s --\n",
      "-- Trial 379/600. Seed: 78. Data size: 10000. Running... Done: 1.16s --\n",
      "-- Trial 380/600. Seed: 79. Data size: 10000. Running... Done: 1.14s --\n",
      "-- Trial 381/600. Seed: 80. Data size: 10000. Running... Done: 1.20s --\n",
      "-- Trial 382/600. Seed: 81. Data size: 10000. Running... Done: 1.14s --\n",
      "-- Trial 383/600. Seed: 82. Data size: 10000. Running... Done: 1.16s --\n",
      "-- Trial 384/600. Seed: 83. Data size: 10000. Running... Done: 1.15s --\n",
      "-- Trial 385/600. Seed: 84. Data size: 10000. Running... Done: 1.16s --\n",
      "-- Trial 386/600. Seed: 85. Data size: 10000. Running... Done: 1.14s --\n",
      "-- Trial 387/600. Seed: 86. Data size: 10000. Running... Done: 1.12s --\n",
      "-- Trial 388/600. Seed: 87. Data size: 10000. Running... Done: 1.14s --\n",
      "-- Trial 389/600. Seed: 88. Data size: 10000. Running... Done: 1.12s --\n",
      "-- Trial 390/600. Seed: 89. Data size: 10000. Running... Done: 1.13s --\n",
      "-- Trial 391/600. Seed: 90. Data size: 10000. Running... Done: 1.14s --\n",
      "-- Trial 392/600. Seed: 91. Data size: 10000. Running... Done: 1.12s --\n",
      "-- Trial 393/600. Seed: 92. Data size: 10000. Running... Done: 1.15s --\n",
      "-- Trial 394/600. Seed: 93. Data size: 10000. Running... Done: 1.15s --\n",
      "-- Trial 395/600. Seed: 94. Data size: 10000. Running... Done: 1.16s --\n",
      "-- Trial 396/600. Seed: 95. Data size: 10000. Running... Done: 1.13s --\n",
      "-- Trial 397/600. Seed: 96. Data size: 10000. Running... Done: 1.11s --\n",
      "-- Trial 398/600. Seed: 97. Data size: 10000. Running... Done: 1.17s --\n",
      "-- Trial 399/600. Seed: 98. Data size: 10000. Running... Done: 1.13s --\n",
      "-- Trial 400/600. Seed: 99. Data size: 10000. Running... Done: 1.14s --\n",
      "-- Trial 401/600. Seed: 0. Data size: 100000. Running... Done: 14.03s --\n",
      "-- Trial 402/600. Seed: 1. Data size: 100000. Running... Done: 13.55s --\n",
      "-- Trial 403/600. Seed: 2. Data size: 100000. Running... Done: 13.54s --\n",
      "-- Trial 404/600. Seed: 3. Data size: 100000. Running... Done: 13.59s --\n",
      "-- Trial 405/600. Seed: 4. Data size: 100000. Running... Done: 13.24s --\n",
      "-- Trial 406/600. Seed: 5. Data size: 100000. Running... Done: 13.39s --\n",
      "-- Trial 407/600. Seed: 6. Data size: 100000. Running... Done: 13.47s --\n",
      "-- Trial 408/600. Seed: 7. Data size: 100000. Running... Done: 13.54s --\n",
      "-- Trial 409/600. Seed: 8. Data size: 100000. Running... Done: 13.68s --\n",
      "-- Trial 410/600. Seed: 9. Data size: 100000. Running... Done: 13.47s --\n",
      "-- Trial 411/600. Seed: 10. Data size: 100000. Running... Done: 13.49s --\n",
      "-- Trial 412/600. Seed: 11. Data size: 100000. Running... Done: 13.29s --\n",
      "-- Trial 413/600. Seed: 12. Data size: 100000. Running... Done: 14.41s --\n",
      "-- Trial 414/600. Seed: 13. Data size: 100000. Running... Done: 13.97s --\n",
      "-- Trial 415/600. Seed: 14. Data size: 100000. Running... Done: 13.45s --\n",
      "-- Trial 416/600. Seed: 15. Data size: 100000. Running... Done: 13.52s --\n",
      "-- Trial 417/600. Seed: 16. Data size: 100000. Running... Done: 13.47s --\n",
      "-- Trial 418/600. Seed: 17. Data size: 100000. Running... Done: 13.67s --\n",
      "-- Trial 419/600. Seed: 18. Data size: 100000. Running... Done: 13.79s --\n",
      "-- Trial 420/600. Seed: 19. Data size: 100000. Running... Done: 13.54s --\n",
      "-- Trial 421/600. Seed: 20. Data size: 100000. Running... Done: 13.39s --\n",
      "-- Trial 422/600. Seed: 21. Data size: 100000. Running... Done: 13.79s --\n",
      "-- Trial 423/600. Seed: 22. Data size: 100000. Running... Done: 13.74s --\n",
      "-- Trial 424/600. Seed: 23. Data size: 100000. Running... Done: 13.36s --\n",
      "-- Trial 425/600. Seed: 24. Data size: 100000. Running... Done: 13.56s --\n",
      "-- Trial 426/600. Seed: 25. Data size: 100000. Running... Done: 13.52s --\n",
      "-- Trial 427/600. Seed: 26. Data size: 100000. Running... Done: 13.44s --\n",
      "-- Trial 428/600. Seed: 27. Data size: 100000. Running... Done: 13.30s --\n",
      "-- Trial 429/600. Seed: 28. Data size: 100000. Running... Done: 13.42s --\n",
      "-- Trial 430/600. Seed: 29. Data size: 100000. Running... Done: 13.14s --\n",
      "-- Trial 431/600. Seed: 30. Data size: 100000. Running... Done: 13.54s --\n",
      "-- Trial 432/600. Seed: 31. Data size: 100000. Running... Done: 13.39s --\n",
      "-- Trial 433/600. Seed: 32. Data size: 100000. Running... Done: 13.32s --\n",
      "-- Trial 434/600. Seed: 33. Data size: 100000. Running... Done: 13.70s --\n",
      "-- Trial 435/600. Seed: 34. Data size: 100000. Running... Done: 13.29s --\n",
      "-- Trial 436/600. Seed: 35. Data size: 100000. Running... Done: 13.71s --\n",
      "-- Trial 437/600. Seed: 36. Data size: 100000. Running... Done: 13.43s --\n",
      "-- Trial 438/600. Seed: 37. Data size: 100000. Running... Done: 13.47s --\n",
      "-- Trial 439/600. Seed: 38. Data size: 100000. Running... Done: 13.55s --\n",
      "-- Trial 440/600. Seed: 39. Data size: 100000. Running... Done: 13.20s --\n",
      "-- Trial 441/600. Seed: 40. Data size: 100000. Running... Done: 13.68s --\n",
      "-- Trial 442/600. Seed: 41. Data size: 100000. Running... Done: 13.59s --\n",
      "-- Trial 443/600. Seed: 42. Data size: 100000. Running... Done: 13.40s --\n",
      "-- Trial 444/600. Seed: 43. Data size: 100000. Running... Done: 13.20s --\n",
      "-- Trial 445/600. Seed: 44. Data size: 100000. Running... Done: 13.51s --\n",
      "-- Trial 446/600. Seed: 45. Data size: 100000. Running... Done: 13.49s --\n",
      "-- Trial 447/600. Seed: 46. Data size: 100000. Running... Done: 13.44s --\n",
      "-- Trial 448/600. Seed: 47. Data size: 100000. Running... Done: 13.59s --\n",
      "-- Trial 449/600. Seed: 48. Data size: 100000. Running... Done: 13.44s --\n",
      "-- Trial 450/600. Seed: 49. Data size: 100000. Running... Done: 13.53s --\n",
      "-- Trial 451/600. Seed: 50. Data size: 100000. Running... Done: 13.31s --\n",
      "-- Trial 452/600. Seed: 51. Data size: 100000. Running... Done: 13.25s --\n",
      "-- Trial 453/600. Seed: 52. Data size: 100000. Running... Done: 13.36s --\n",
      "-- Trial 454/600. Seed: 53. Data size: 100000. Running... Done: 13.48s --\n",
      "-- Trial 455/600. Seed: 54. Data size: 100000. Running... Done: 13.47s --\n",
      "-- Trial 456/600. Seed: 55. Data size: 100000. Running... Done: 13.28s --\n",
      "-- Trial 457/600. Seed: 56. Data size: 100000. Running... Done: 13.47s --\n",
      "-- Trial 458/600. Seed: 57. Data size: 100000. Running... Done: 13.39s --\n",
      "-- Trial 459/600. Seed: 58. Data size: 100000. Running... Done: 13.58s --\n",
      "-- Trial 460/600. Seed: 59. Data size: 100000. Running... Done: 13.46s --\n",
      "-- Trial 461/600. Seed: 60. Data size: 100000. Running... Done: 13.81s --\n",
      "-- Trial 462/600. Seed: 61. Data size: 100000. Running... Done: 13.34s --\n",
      "-- Trial 463/600. Seed: 62. Data size: 100000. Running... Done: 13.68s --\n",
      "-- Trial 464/600. Seed: 63. Data size: 100000. Running... Done: 13.47s --\n",
      "-- Trial 465/600. Seed: 64. Data size: 100000. Running... Done: 13.43s --\n",
      "-- Trial 466/600. Seed: 65. Data size: 100000. Running... Done: 13.38s --\n",
      "-- Trial 467/600. Seed: 66. Data size: 100000. Running... Done: 13.36s --\n",
      "-- Trial 468/600. Seed: 67. Data size: 100000. Running... Done: 13.34s --\n",
      "-- Trial 469/600. Seed: 68. Data size: 100000. Running... Done: 13.26s --\n",
      "-- Trial 470/600. Seed: 69. Data size: 100000. Running... Done: 13.43s --\n",
      "-- Trial 471/600. Seed: 70. Data size: 100000. Running... Done: 13.26s --\n",
      "-- Trial 472/600. Seed: 71. Data size: 100000. Running... Done: 13.65s --\n",
      "-- Trial 473/600. Seed: 72. Data size: 100000. Running... Done: 13.60s --\n",
      "-- Trial 474/600. Seed: 73. Data size: 100000. Running... Done: 13.31s --\n",
      "-- Trial 475/600. Seed: 74. Data size: 100000. Running... Done: 13.37s --\n",
      "-- Trial 476/600. Seed: 75. Data size: 100000. Running... Done: 13.49s --\n",
      "-- Trial 477/600. Seed: 76. Data size: 100000. Running... Done: 13.33s --\n",
      "-- Trial 478/600. Seed: 77. Data size: 100000. Running... Done: 13.56s --\n",
      "-- Trial 479/600. Seed: 78. Data size: 100000. Running... Done: 13.57s --\n",
      "-- Trial 480/600. Seed: 79. Data size: 100000. Running... Done: 13.43s --\n",
      "-- Trial 481/600. Seed: 80. Data size: 100000. Running... Done: 13.36s --\n",
      "-- Trial 482/600. Seed: 81. Data size: 100000. Running... Done: 13.43s --\n",
      "-- Trial 483/600. Seed: 82. Data size: 100000. Running... Done: 13.61s --\n",
      "-- Trial 484/600. Seed: 83. Data size: 100000. Running... Done: 13.59s --\n",
      "-- Trial 485/600. Seed: 84. Data size: 100000. Running... Done: 13.24s --\n",
      "-- Trial 486/600. Seed: 85. Data size: 100000. Running... Done: 13.62s --\n",
      "-- Trial 487/600. Seed: 86. Data size: 100000. Running... Done: 13.37s --\n",
      "-- Trial 488/600. Seed: 87. Data size: 100000. Running... Done: 13.55s --\n",
      "-- Trial 489/600. Seed: 88. Data size: 100000. Running... Done: 13.60s --\n",
      "-- Trial 490/600. Seed: 89. Data size: 100000. Running... Done: 13.52s --\n",
      "-- Trial 491/600. Seed: 90. Data size: 100000. Running... Done: 13.28s --\n",
      "-- Trial 492/600. Seed: 91. Data size: 100000. Running... Done: 13.31s --\n",
      "-- Trial 493/600. Seed: 92. Data size: 100000. Running... Done: 13.23s --\n",
      "-- Trial 494/600. Seed: 93. Data size: 100000. Running... Done: 13.53s --\n",
      "-- Trial 495/600. Seed: 94. Data size: 100000. Running... Done: 13.36s --\n",
      "-- Trial 496/600. Seed: 95. Data size: 100000. Running... Done: 13.47s --\n",
      "-- Trial 497/600. Seed: 96. Data size: 100000. Running... Done: 13.29s --\n",
      "-- Trial 498/600. Seed: 97. Data size: 100000. Running... Done: 13.48s --\n",
      "-- Trial 499/600. Seed: 98. Data size: 100000. Running... Done: 13.46s --\n",
      "-- Trial 500/600. Seed: 99. Data size: 100000. Running... Done: 13.50s --\n",
      "-- Trial 501/600. Seed: 0. Data size: 1000000. Running... Done: 172.53s --\n",
      "-- Trial 502/600. Seed: 1. Data size: 1000000. Running... Done: 169.30s --\n",
      "-- Trial 503/600. Seed: 2. Data size: 1000000. Running... Done: 168.85s --\n",
      "-- Trial 504/600. Seed: 3. Data size: 1000000. Running... Done: 169.93s --\n",
      "-- Trial 505/600. Seed: 4. Data size: 1000000. Running... Done: 170.05s --\n",
      "-- Trial 506/600. Seed: 5. Data size: 1000000. Running... Done: 169.70s --\n",
      "-- Trial 507/600. Seed: 6. Data size: 1000000. Running... Done: 171.06s --\n",
      "-- Trial 508/600. Seed: 7. Data size: 1000000. Running... Done: 168.07s --\n",
      "-- Trial 509/600. Seed: 8. Data size: 1000000. Running... Done: 166.61s --\n",
      "-- Trial 510/600. Seed: 9. Data size: 1000000. Running... Done: 170.45s --\n",
      "-- Trial 511/600. Seed: 10. Data size: 1000000. Running... Done: 168.39s --\n",
      "-- Trial 512/600. Seed: 11. Data size: 1000000. Running... Done: 168.70s --\n",
      "-- Trial 513/600. Seed: 12. Data size: 1000000. Running... Done: 173.40s --\n",
      "-- Trial 514/600. Seed: 13. Data size: 1000000. Running... Done: 167.43s --\n",
      "-- Trial 515/600. Seed: 14. Data size: 1000000. Running... Done: 166.24s --\n",
      "-- Trial 516/600. Seed: 15. Data size: 1000000. Running... Done: 167.75s --\n",
      "-- Trial 517/600. Seed: 16. Data size: 1000000. Running... Done: 169.20s --\n",
      "-- Trial 518/600. Seed: 17. Data size: 1000000. Running... Done: 169.51s --\n",
      "-- Trial 519/600. Seed: 18. Data size: 1000000. Running... Done: 168.18s --\n",
      "-- Trial 520/600. Seed: 19. Data size: 1000000. Running... Done: 168.51s --\n",
      "-- Trial 521/600. Seed: 20. Data size: 1000000. Running... Done: 168.00s --\n",
      "-- Trial 522/600. Seed: 21. Data size: 1000000. Running... Done: 168.73s --\n",
      "-- Trial 523/600. Seed: 22. Data size: 1000000. Running... Done: 166.99s --\n",
      "-- Trial 524/600. Seed: 23. Data size: 1000000. Running... Done: 180.80s --\n",
      "-- Trial 525/600. Seed: 24. Data size: 1000000. Running... Done: 178.88s --\n",
      "-- Trial 526/600. Seed: 25. Data size: 1000000. Running... Done: 177.95s --\n",
      "-- Trial 527/600. Seed: 26. Data size: 1000000. Running... Done: 178.08s --\n",
      "-- Trial 528/600. Seed: 27. Data size: 1000000. Running... Done: 177.95s --\n",
      "-- Trial 529/600. Seed: 28. Data size: 1000000. Running... Done: 182.29s --\n",
      "-- Trial 530/600. Seed: 29. Data size: 1000000. Running... Done: 181.84s --\n",
      "-- Trial 531/600. Seed: 30. Data size: 1000000. Running... Done: 177.90s --\n",
      "-- Trial 532/600. Seed: 31. Data size: 1000000. Running... Done: 181.69s --\n",
      "-- Trial 533/600. Seed: 32. Data size: 1000000. Running... Done: 179.37s --\n",
      "-- Trial 534/600. Seed: 33. Data size: 1000000. Running... Done: 178.70s --\n",
      "-- Trial 535/600. Seed: 34. Data size: 1000000. Running... Done: 180.37s --\n",
      "-- Trial 536/600. Seed: 35. Data size: 1000000. Running... Done: 178.39s --\n",
      "-- Trial 537/600. Seed: 36. Data size: 1000000. Running... Done: 177.80s --\n",
      "-- Trial 538/600. Seed: 37. Data size: 1000000. Running... Done: 177.15s --\n",
      "-- Trial 539/600. Seed: 38. Data size: 1000000. Running... Done: 176.45s --\n",
      "-- Trial 540/600. Seed: 39. Data size: 1000000. Running... Done: 177.79s --\n",
      "-- Trial 541/600. Seed: 40. Data size: 1000000. Running... Done: 181.12s --\n",
      "-- Trial 542/600. Seed: 41. Data size: 1000000. Running... Done: 178.51s --\n",
      "-- Trial 543/600. Seed: 42. Data size: 1000000. Running... Done: 179.09s --\n",
      "-- Trial 544/600. Seed: 43. Data size: 1000000. Running... Done: 176.72s --\n",
      "-- Trial 545/600. Seed: 44. Data size: 1000000. Running... Done: 178.78s --\n",
      "-- Trial 546/600. Seed: 45. Data size: 1000000. Running... Done: 180.21s --\n",
      "-- Trial 547/600. Seed: 46. Data size: 1000000. Running... Done: 179.41s --\n",
      "-- Trial 548/600. Seed: 47. Data size: 1000000. Running... Done: 180.75s --\n",
      "-- Trial 549/600. Seed: 48. Data size: 1000000. Running... Done: 178.81s --\n",
      "-- Trial 550/600. Seed: 49. Data size: 1000000. Running... Done: 177.60s --\n",
      "-- Trial 551/600. Seed: 50. Data size: 1000000. Running... Done: 181.08s --\n",
      "-- Trial 552/600. Seed: 51. Data size: 1000000. Running... Done: 177.98s --\n",
      "-- Trial 553/600. Seed: 52. Data size: 1000000. Running... Done: 178.78s --\n",
      "-- Trial 554/600. Seed: 53. Data size: 1000000. Running... Done: 177.99s --\n",
      "-- Trial 555/600. Seed: 54. Data size: 1000000. Running... Done: 177.78s --\n",
      "-- Trial 556/600. Seed: 55. Data size: 1000000. Running... Done: 180.30s --\n",
      "-- Trial 557/600. Seed: 56. Data size: 1000000. Running... Done: 181.70s --\n",
      "-- Trial 558/600. Seed: 57. Data size: 1000000. Running... Done: 177.34s --\n",
      "-- Trial 559/600. Seed: 58. Data size: 1000000. Running... Done: 168.15s --\n",
      "-- Trial 560/600. Seed: 59. Data size: 1000000. Running... Done: 168.50s --\n",
      "-- Trial 561/600. Seed: 60. Data size: 1000000. Running... Done: 169.10s --\n",
      "-- Trial 562/600. Seed: 61. Data size: 1000000. Running... Done: 168.03s --\n",
      "-- Trial 563/600. Seed: 62. Data size: 1000000. Running... Done: 166.75s --\n",
      "-- Trial 564/600. Seed: 63. Data size: 1000000. Running... Done: 169.37s --\n",
      "-- Trial 565/600. Seed: 64. Data size: 1000000. Running... Done: 168.77s --\n",
      "-- Trial 566/600. Seed: 65. Data size: 1000000. Running... Done: 167.32s --\n",
      "-- Trial 567/600. Seed: 66. Data size: 1000000. Running... Done: 168.09s --\n",
      "-- Trial 568/600. Seed: 67. Data size: 1000000. Running... Done: 168.79s --\n",
      "-- Trial 569/600. Seed: 68. Data size: 1000000. Running... Done: 169.85s --\n",
      "-- Trial 570/600. Seed: 69. Data size: 1000000. Running... Done: 169.74s --\n",
      "-- Trial 571/600. Seed: 70. Data size: 1000000. Running... Done: 168.47s --\n",
      "-- Trial 572/600. Seed: 71. Data size: 1000000. Running... Done: 170.39s --\n",
      "-- Trial 573/600. Seed: 72. Data size: 1000000. Running... Done: 169.44s --\n",
      "-- Trial 574/600. Seed: 73. Data size: 1000000. Running... Done: 169.05s --\n",
      "-- Trial 575/600. Seed: 74. Data size: 1000000. Running... Done: 169.17s --\n",
      "-- Trial 576/600. Seed: 75. Data size: 1000000. Running... Done: 167.52s --\n",
      "-- Trial 577/600. Seed: 76. Data size: 1000000. Running... Done: 168.18s --\n",
      "-- Trial 578/600. Seed: 77. Data size: 1000000. Running... Done: 180.33s --\n",
      "-- Trial 579/600. Seed: 78. Data size: 1000000. Running... Done: 198.79s --\n",
      "-- Trial 580/600. Seed: 79. Data size: 1000000. Running... Done: 186.30s --\n",
      "-- Trial 581/600. Seed: 80. Data size: 1000000. Running... Done: 185.70s --\n",
      "-- Trial 582/600. Seed: 81. Data size: 1000000. Running... Done: 178.42s --\n",
      "-- Trial 583/600. Seed: 82. Data size: 1000000. Running... Done: 171.40s --\n",
      "-- Trial 584/600. Seed: 83. Data size: 1000000. Running... Done: 170.91s --\n",
      "-- Trial 585/600. Seed: 84. Data size: 1000000. Running... Done: 169.87s --\n",
      "-- Trial 586/600. Seed: 85. Data size: 1000000. Running... Done: 171.48s --\n",
      "-- Trial 587/600. Seed: 86. Data size: 1000000. Running... Done: 172.99s --\n",
      "-- Trial 588/600. Seed: 87. Data size: 1000000. Running... Done: 176.12s --\n",
      "-- Trial 589/600. Seed: 88. Data size: 1000000. Running... Done: 174.54s --\n",
      "-- Trial 590/600. Seed: 89. Data size: 1000000. Running... Done: 168.11s --\n",
      "-- Trial 591/600. Seed: 90. Data size: 1000000. Running... Done: 169.84s --\n",
      "-- Trial 592/600. Seed: 91. Data size: 1000000. Running... Done: 168.16s --\n",
      "-- Trial 593/600. Seed: 92. Data size: 1000000. Running... Done: 166.34s --\n",
      "-- Trial 594/600. Seed: 93. Data size: 1000000. Running... Done: 166.85s --\n",
      "-- Trial 595/600. Seed: 94. Data size: 1000000. Running... Done: 168.34s --\n",
      "-- Trial 596/600. Seed: 95. Data size: 1000000. Running... Done: 168.50s --\n",
      "-- Trial 597/600. Seed: 96. Data size: 1000000. Running... Done: 166.69s --\n",
      "-- Trial 598/600. Seed: 97. Data size: 1000000. Running... Done: 169.15s --\n",
      "-- Trial 599/600. Seed: 98. Data size: 1000000. Running... Done: 167.13s --\n",
      "-- Trial 600/600. Seed: 99. Data size: 1000000. Running... Done: 173.53s --\n"
     ]
    }
   ],
   "source": [
    "n_seeds = 100 # number of randomized trials for each configuration\n",
    "n_elements_list = [int(el) for el in [1e1, 1e2, 1e3, 1e4, 1e5, 1e6]]\n",
    "loads = [2_000, 10_000]\n",
    "element_max_size = int(2e6)\n",
    "\n",
    "total_n_trials = n_seeds * len(n_elements_list)\n",
    "trial_count = 0\n",
    "\n",
    "with open('benchmarks.csv', 'w', newline='') as file:\n",
    "    \n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Abstract Data Type\", \"Implementation\", \"Data Size\", \"Runtime\", \"Operation\"])\n",
    "\n",
    "    for n_elements in n_elements_list:\n",
    "\n",
    "            for seed in range(n_seeds):\n",
    "                trial_count += 1\n",
    "\n",
    "                print(f'-- Trial {trial_count}/{total_n_trials}. Seed: {seed}. Data size: {n_elements}. Running...', end=' ')\n",
    "\n",
    "                trial_start = time.time()\n",
    "                \n",
    "                random.seed(seed)\n",
    "                trial_insert_data = [random.randint(1, element_max_size) for i in range(n_elements)] \n",
    "\n",
    "                random.seed(seed+1)\n",
    "                trial_lookup_not_inserted_data = [random.randint(1, element_max_size) for i in range(n_elements)] \n",
    "\n",
    "                # SortedList\n",
    "                sortedcontainers_list = SortedcontainersSortedList()\n",
    "                pyskiplist = PySkipList()\n",
    "                \n",
    "                # SortedDict\n",
    "                our_teleport_list = TeleportList()\n",
    "                sortedcontainers_sorted_dict = SortedcontainersSortedDict()\n",
    "\n",
    "                # INSERT\n",
    "\n",
    "                t = %timeit -q -o -r 1 -n 1 insert_sortedcontainers_list(sortedcontainers_list, trial_insert_data)\n",
    "                writer.writerow(['SortedList', f'SortedContainersSortedList', n_elements, f'{t.average:.8f}', 'insert'])\n",
    "                \n",
    "                t = %timeit -q -o -r 1 -n 1 insert_pyskiplist(pyskiplist, trial_insert_data)\n",
    "                writer.writerow(['SortedList', f'PySkipList', n_elements, f'{t.average:.8f}', 'insert'])\n",
    "                \n",
    "                t = %timeit -q -o -r 1 -n 1 insert_our(our_teleport_list, trial_insert_data)\n",
    "                writer.writerow(['SortedDict', f'RoaringTeleportList', n_elements, f'{t.average:.8f}', 'insert'])\n",
    "                \n",
    "                t = %timeit -q -o -r 1 -n 1 insert_sortedcontainers_dict(sortedcontainers_sorted_dict, trial_insert_data)\n",
    "                writer.writerow(['SortedDict', f'SortedContainersSortedDict', n_elements, f'{t.average:.8f}', 'insert'])\n",
    "\n",
    "                # LOOKUP\n",
    "                \n",
    "                # lookup data that was inserted\n",
    "                t = %timeit -q -o -r 1 -n 1 lookup_sortedcontainers_list(sortedcontainers_list, trial_insert_data)\n",
    "                writer.writerow(['SortedList', f'SortedContainersSortedList', n_elements, f'{t.average:.8f}', 'lookup_inserted'])\n",
    "                \n",
    "                t = %timeit -q -o -r 1 -n 1 lookup_pyskiplist(pyskiplist, trial_insert_data)\n",
    "                writer.writerow(['SortedList', f'PySkipList', n_elements, f'{t.average:.8f}', 'lookup_inserted'])\n",
    "                \n",
    "                t = %timeit -q -o -r 1 -n 1 lookup_our(our_teleport_list, trial_insert_data)\n",
    "                writer.writerow(['SortedDict', f'RoaringTeleportList', n_elements, f'{t.average:.8f}', 'lookup_inserted'])\n",
    "                \n",
    "                t = %timeit -q -o -r 1 -n 1 lookup_sortedcontainers_dict(sortedcontainers_sorted_dict, trial_insert_data)\n",
    "                writer.writerow(['SortedDict', f'SortedContainersSortedDict', n_elements, f'{t.average:.8f}', 'lookup_inserted'])\n",
    "\n",
    "                # lookup data that was NOT inserted\n",
    "                t = %timeit -q -o -r 1 -n 1 lookup_sortedcontainers_list(sortedcontainers_list, trial_lookup_not_inserted_data)\n",
    "                writer.writerow(['SortedList', f'SortedContainersSortedList', n_elements, f'{t.average:.8f}', 'lookup_not_inserted'])\n",
    "                \n",
    "                t = %timeit -q -o -r 1 -n 1 lookup_pyskiplist(pyskiplist, trial_lookup_not_inserted_data)\n",
    "                writer.writerow(['SortedList', f'PySkipList', n_elements, f'{t.average:.8f}', 'lookup_not_inserted'])\n",
    "                \n",
    "                t = %timeit -q -o -r 1 -n 1 lookup_our(our_teleport_list, trial_lookup_not_inserted_data)\n",
    "                writer.writerow(['SortedDict', f'RoaringTeleportList', n_elements, f'{t.average:.8f}', 'lookup_not_inserted'])\n",
    "                \n",
    "                t = %timeit -q -o -r 1 -n 1 lookup_sortedcontainers_dict(sortedcontainers_sorted_dict, trial_lookup_not_inserted_data)\n",
    "                writer.writerow(['SortedDict', f'SortedContainersSortedDict', n_elements, f'{t.average:.8f}', 'lookup_not_inserted'])\n",
    "                \n",
    "                for load in loads:\n",
    "                    \n",
    "                    # SortedList\n",
    "                    our_splitlist = SplitList(load)\n",
    "                    our_mono_splist = MonoboundSplitList(load)\n",
    "                    \n",
    "                    # SortedDict\n",
    "                    our_roaring_splist = RoaringSplitList(load)\n",
    "                    \n",
    "                    # INSERT\n",
    "\n",
    "                    t = %timeit -q -o -r 1 -n 1 insert_our(our_splitlist, trial_insert_data)\n",
    "                    writer.writerow(['SortedList', f'SplitList-{load}', n_elements, f'{t.average:.8f}', 'insert'])\n",
    "\n",
    "                    t = %timeit -q -o -r 1 -n 1 insert_our(our_mono_splist, trial_insert_data)\n",
    "                    writer.writerow(['SortedList', f'MonoboundSplitList-{load}', n_elements, f'{t.average:.8f}', 'insert'])\n",
    "\n",
    "                    t = %timeit -q -o -r 1 -n 1 insert_our(our_roaring_splist, trial_insert_data)\n",
    "                    writer.writerow(['SortedDict', f'RoaringSplitList-{load}', n_elements, f'{t.average:.8f}', 'insert'])\n",
    "\n",
    "                    # LOOKUP\n",
    "                    \n",
    "                    # lookup data that was inserted\n",
    "                    t = %timeit -q -o -r 1 -n 1 lookup_our(our_splitlist, trial_insert_data)\n",
    "                    writer.writerow(['SortedList', f'SplitList-{load}', n_elements, f'{t.average:.8f}', 'lookup_inserted'])\n",
    "\n",
    "                    t = %timeit -q -o -r 1 -n 1 lookup_our(our_mono_splist, trial_insert_data)\n",
    "                    writer.writerow(['SortedList', f'MonoboundSplitList-{load}', n_elements, f'{t.average:.8f}', 'lookup_inserted'])\n",
    "\n",
    "                    t = %timeit -q -o -r 1 -n 1 lookup_our(our_roaring_splist, trial_insert_data)\n",
    "                    writer.writerow(['SortedDict', f'RoaringSplitList-{load}', n_elements, f'{t.average:.8f}', 'lookup_inserted'])\n",
    "                    \n",
    "                    # lookup data that was NOT inserted\n",
    "                    t = %timeit -q -o -r 1 -n 1 lookup_our(our_splitlist, trial_lookup_not_inserted_data)\n",
    "                    writer.writerow(['SortedList', f'SplitList-{load}', n_elements, f'{t.average:.8f}', 'lookup_not_inserted'])\n",
    "\n",
    "                    t = %timeit -q -o -r 1 -n 1 lookup_our(our_mono_splist, trial_lookup_not_inserted_data)\n",
    "                    writer.writerow(['SortedList', f'MonoboundSplitList-{load}', n_elements, f'{t.average:.8f}', 'lookup_not_inserted'])\n",
    "\n",
    "                    t = %timeit -q -o -r 1 -n 1 lookup_our(our_roaring_splist, trial_lookup_not_inserted_data)\n",
    "                    writer.writerow(['SortedDict', f'RoaringSplitList-{load}', n_elements, f'{t.average:.8f}', 'lookup_not_inserted'])\n",
    "                    \n",
    "                    # small cleanup\n",
    "                    del our_splitlist\n",
    "                    del our_mono_splist\n",
    "                    del our_roaring_splist\n",
    "                                        \n",
    "                # cleanup\n",
    "                del trial_insert_data\n",
    "                del trial_lookup_not_inserted_data\n",
    "                \n",
    "                del sortedcontainers_list\n",
    "                del pyskiplist\n",
    "                del our_teleport_list\n",
    "                del sortedcontainers_sorted_dict\n",
    "\n",
    "                gc.collect() # Sir garbage collector, please clean up this mess.\n",
    "\n",
    "                trial_total = time.time() - trial_start\n",
    "\n",
    "                print(f'Done: {trial_total:.2f}s --')\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
